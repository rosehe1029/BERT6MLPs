{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QNLI : Question Natural Language Inference\n",
    "\n",
    "The Question Natural Language Inference(QNLI) task is a sentence pair classification task. It consists of sentence pairs drawn from the Stanford Question Answering Dataset(SQUAD) and reframed as binary textual entailment.\n",
    "\n",
    "See [Squad website](https://rajpurkar.github.io/SQuAD-explorer/) for more info on the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sys.path.append(\"../\") \n",
    "from bert_sklearn import BertClassifier\n",
    "from bert_sklearn import load_model\n",
    "\n",
    "DATADIR = os.getcwd() + '/glue_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting QNLI...\n",
      "\tCompleted!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 download_glue_data.py --data_dir glue_data --tasks QNLI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNLI train data size: 108436 \n",
      "QNLI dev data size: 5732 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "QNLI train data size: 108436 \n",
    "QNLI dev data size: 5732 \n",
    "\"\"\"\n",
    "def read_tsv(filename,quotechar=None):\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        return list(csv.reader(f,delimiter=\"\\t\",quotechar=quotechar))\n",
    "    \n",
    "def get_qnli_df(filename):\n",
    "    rows = read_tsv(filename)\n",
    "    df=pd.DataFrame(rows[1:],columns=rows[0])\n",
    "    df=df[['question','sentence','label']]\n",
    "    df = df[pd.notnull(df['label'])]\n",
    "    df.columns=['text_a','text_b','label']\n",
    "    return df\n",
    "\n",
    "def get_qnli_data(train_file = DATADIR+'/QNLI/train.tsv', \n",
    "                   dev_file =  DATADIR+'/QNLI/dev.tsv'):\n",
    "    \n",
    "    train = get_qnli_df(train_file)\n",
    "    print(\"QNLI train data size: %d \"%(len(train)))\n",
    "    dev = get_qnli_df(dev_file)\n",
    "    print(\"QNLI dev data size: %d \"%(len(dev)))\n",
    "\n",
    "    label_list = np.unique(train['label'].values)\n",
    "    return train,dev,label_list  \n",
    "                  \n",
    "train,dev,label_list =  get_qnli_data()             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entailment' 'not_entailment']\n"
     ]
    }
   ],
   "source": [
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>Immediately behind the basilica is the Grotto,...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>It is a replica of the grotto at Lourdes, Fran...</td>\n",
       "      <td>not_entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>Atop the Main Building's gold dome is a golden...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>Next to the Main Building is the Basilica of t...</td>\n",
       "      <td>not_entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When did the Scholastic Magazine of Notre dame...</td>\n",
       "      <td>Begun as a one-page journal in September 1876,...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_a  \\\n",
       "0                  What is the Grotto at Notre Dame?   \n",
       "1                  What is the Grotto at Notre Dame?   \n",
       "2  What sits on top of the Main Building at Notre...   \n",
       "3  What sits on top of the Main Building at Notre...   \n",
       "4  When did the Scholastic Magazine of Notre dame...   \n",
       "\n",
       "                                              text_b           label  \n",
       "0  Immediately behind the basilica is the Grotto,...      entailment  \n",
       "1  It is a replica of the grotto at Lourdes, Fran...  not_entailment  \n",
       "2  Atop the Main Building's gold dome is a golden...      entailment  \n",
       "3  Next to the Main Building is the Basilica of t...  not_entailment  \n",
       "4  Begun as a one-page journal in September 1876,...      entailment  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn classifier...\n",
      "\n",
      " BertClassifier(bert_model='bert-base-uncased', epochs=4, eval_batch_size=8,\n",
      "        fp16=False, gradient_accumulation_steps=1,\n",
      "        label_list=array(['entailment', 'not_entailment'], dtype=object),\n",
      "        learning_rate=4e-05, local_rank=-1, logfile='bert_sklearn.log',\n",
      "        loss_scale=0, max_seq_length=96, num_mlp_hiddens=500,\n",
      "        num_mlp_layers=0, random_state=42, restore_file=None,\n",
      "        train_batch_size=32, use_cuda=True, validation_fraction=0.05,\n",
      "        warmup_proportion=0.1) \n",
      "\n",
      "Loading bert-base-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "train data size: 103015, validation data size: 5421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3220/3220 [44:42<00:00,  1.48it/s, loss=0.406]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss : 0.4062, Val loss: 0.3141, Val accy = 87.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3220/3220 [48:23<00:00,  1.40it/s, loss=0.228]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss : 0.2277, Val loss: 0.3287, Val accy = 87.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3220/3220 [48:07<00:00,  1.42it/s, loss=0.121]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss : 0.1213, Val loss: 0.4244, Val accy = 87.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3220/3220 [47:51<00:00,  1.37it/s, loss=0.08]  \n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train loss : 0.0800, Val loss: 0.5211, Val accy = 87.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.57%\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    entailment       0.88      0.89      0.89      2866\n",
      "not_entailment       0.89      0.88      0.89      2866\n",
      "\n",
      "     micro avg       0.89      0.89      0.89      5732\n",
      "     macro avg       0.89      0.89      0.89      5732\n",
      "  weighted avg       0.89      0.89      0.89      5732\n",
      "\n",
      "CPU times: user 2h 55s, sys: 1h 13min 12s, total: 3h 14min 8s\n",
      "Wall time: 3h 14min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train = train[['text_a','text_b']]\n",
    "y_train = train['label']\n",
    "\n",
    "# define model\n",
    "model = BertClassifier()\n",
    "model.label_list = label_list\n",
    "model.epochs = 3\n",
    "model.validation_fraction = 0.05\n",
    "model.learning_rate = 4e-5\n",
    "model.max_seq_length = 96\n",
    "\n",
    "print('\\n',model,'\\n')\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# test model on dev\n",
    "test = dev\n",
    "X_test = test[['text_a','text_b']]\n",
    "y_test = test['label']\n",
    "\n",
    "# make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy: %0.2f%%\"%(metrics.accuracy_score(y_pred,y_test) * 100))\n",
    "print(classification_report(y_test, y_pred, target_names=label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with a MLP..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn classifier...\n",
      "\n",
      " BertClassifier(bert_model='bert-base-uncased', epochs=4, eval_batch_size=8,\n",
      "        fp16=False, gradient_accumulation_steps=1,\n",
      "        label_list=array(['entailment', 'not_entailment'], dtype=object),\n",
      "        learning_rate=4e-05, local_rank=-1, logfile='bert_sklearn.log',\n",
      "        loss_scale=0, max_seq_length=96, num_mlp_hiddens=500,\n",
      "        num_mlp_layers=4, random_state=42, restore_file=None,\n",
      "        train_batch_size=32, use_cuda=True, validation_fraction=0.05,\n",
      "        warmup_proportion=0.1) \n",
      "\n",
      "Loading bert-base-uncased model...\n",
      "Using mlp with D=768,H=500,K=2,n=4\n",
      "train data size: 103015, validation data size: 5421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3219/3219 [44:29<00:00,  1.17it/s, loss=0.419]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss : 0.4188, Val loss: 0.3102, Val accy = 87.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3219/3219 [48:26<00:00,  1.14it/s, loss=0.239]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss : 0.2391, Val loss: 0.3060, Val accy = 87.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3219/3219 [48:25<00:00,  1.12it/s, loss=0.139]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss : 0.1390, Val loss: 0.3463, Val accy = 88.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3219/3219 [48:36<00:00,  1.17it/s, loss=0.0973]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train loss : 0.0973, Val loss: 0.3834, Val accy = 87.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.75%\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    entailment       0.89      0.88      0.89      2866\n",
      "not_entailment       0.89      0.89      0.89      2866\n",
      "\n",
      "     micro avg       0.89      0.89      0.89      5732\n",
      "     macro avg       0.89      0.89      0.89      5732\n",
      "  weighted avg       0.89      0.89      0.89      5732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train = train[['text_a','text_b']]\n",
    "y_train = train['label']\n",
    "\n",
    "# define model\n",
    "model = BertClassifier()\n",
    "model.label_list = label_list\n",
    "model.epochs = 4\n",
    "model.validation_fraction = 0.05\n",
    "model.learning_rate = 4e-5\n",
    "model.max_seq_length = 96\n",
    "model.num_mlp_layers = 4\n",
    "\n",
    "print('\\n',model,'\\n')\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# test model on dev\n",
    "test = dev\n",
    "X_test = test[['text_a','text_b']]\n",
    "y_test = test['label']\n",
    "\n",
    "# make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy: %0.2f%%\"%(metrics.accuracy_score(y_pred,y_test) * 100))\n",
    "print(classification_report(y_test, y_pred, target_names=label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
