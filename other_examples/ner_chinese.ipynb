{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese Named Entity Recognition (NER)\n",
    "\n",
    "data is from https://github.com/ProHiryu/bert-chinese-ner\n",
    "\n",
    "The  data is in a similar format to the **`CoNLL 2003`** shared task with 4 types of `Named Entities` (persons, locations, organizations, and miscellaneous entities). The data is in a [IOB2](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) format.  Each token enitity has a `'B-'` or `'I-'` tags indicating if it is the start of the entity or if the token is inside the annotation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token  tag\n",
    "美     B-LOC\n",
    "国     I-LOC\n",
    "的     O\n",
    "华     B-PER\n",
    "莱     B-PER\n",
    "士     B-PER\n",
    "，     O\n",
    "我     O\n",
    "和     O\n",
    "他     O\n",
    "谈     O\n",
    "笑     O\n",
    "风     O\n",
    "生     O\n",
    "。     O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the `token`, the second column is the `NER` tag. \n",
    "\n",
    "So for the named entity recognition (NER) task our data consists of features, `X`, and labels, `y`:\n",
    "\n",
    "\n",
    "* **`X`** :  a list of list of tokens \n",
    "\n",
    "\n",
    "* **`y`** :  a list of list of NER tags\n",
    "\n",
    "## get data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATADIR=\"ner_chinese\"\n",
    "if test ! -d \"$DATADIR\";then\n",
    "    echo \"Creating $DATADIR dir\"\n",
    "    mkdir \"$DATADIR\"\n",
    "    cd \"$DATADIR\"\n",
    "    wget https://raw.githubusercontent.com/ProHiryu/bert-chinese-ner/master/data/train.txt\n",
    "    wget https://raw.githubusercontent.com/ProHiryu/bert-chinese-ner/master/data/dev.txt\n",
    "    wget https://raw.githubusercontent.com/ProHiryu/bert-chinese-ner/master/data/test.txt\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 50658 sentences, 2169879 tokens\n",
      "Dev data: 4631 sentences, 172601 tokens\n",
      "Test data: 69 sentences, 2294 tokens\n",
      "\n",
      "NER tags/labels:\n",
      " ['B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train data: 50658 sentences, 2169879 tokens\n",
    "Dev data: 4631 sentences, 172601 tokens\n",
    "Test data: 69 sentences, 2294 tokens\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sys.path.append(\"../\") \n",
    "from bert_sklearn import BertTokenClassifier, load_model\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def read_CoNLL2003_format(filename, idx=3):\n",
    "    \"\"\"Read file in CoNLL-2003 shared task format\"\"\"\n",
    "    # read file\n",
    "    lines =  open(filename).read().strip()\n",
    "    \n",
    "    # find sentence-like boundaries\n",
    "    lines = lines.split(\"\\n\\n\")  \n",
    "    \n",
    "     # split on newlines\n",
    "    lines = [line.split(\"\\n\") for line in lines]\n",
    "    \n",
    "    # get tokens\n",
    "    tokens = [[l.split()[0] for l in line] for line in lines]\n",
    "    \n",
    "    # get labels/tags\n",
    "    labels = [[l.split()[idx] for l in line] for line in lines]\n",
    "    \n",
    "    #convert to df\n",
    "    data= {'tokens': tokens, 'labels': labels}\n",
    "    df=pd.DataFrame(data=data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "DATADIR = \"./ner_chinese/\"\n",
    "\n",
    "def get_data(trainfile=DATADIR + \"train.txt\",\n",
    "             devfile=DATADIR + \"dev.txt\",\n",
    "             testfile=DATADIR + \"test.txt\"):\n",
    "\n",
    "    train = read_CoNLL2003_format(trainfile, 1)\n",
    "    print(\"Train data: %d sentences, %d tokens\"%(len(train),len(flatten(train.tokens))))\n",
    "\n",
    "    dev = read_CoNLL2003_format(devfile, 1)\n",
    "    print(\"Dev data: %d sentences, %d tokens\"%(len(dev),len(flatten(dev.tokens))))\n",
    "\n",
    "    test = read_CoNLL2003_format(testfile, 1)\n",
    "    print(\"Test data: %d sentences, %d tokens\"%(len(test),len(flatten(test.tokens))))\n",
    "    \n",
    "    return train, dev, test\n",
    "\n",
    "\n",
    "train, dev, test = get_data()\n",
    "\n",
    "X_train, y_train = train.tokens, train.labels\n",
    "X_dev, y_dev = dev.tokens, dev.labels\n",
    "X_test, y_test = test.tokens, test.labels\n",
    "\n",
    "label_list = np.unique(flatten(y_train))\n",
    "label_list = list(label_list)\n",
    "print(\"\\nNER tags/labels:\\n\", label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[当, 希, 望, 工, 程, 救, 助, 的, 百, 万, 儿, 童, 成, 长, 起, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[藏, 书, 本, 来, 就, 是, 所, 有, 传, 统, 收, 藏, 门, 类, 中, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[因, 有, 关, 日, 寇, 在, 京, 掠, 夺, 文, 物, 详, 情, ，, 藏, ...</td>\n",
       "      <td>[O, O, O, B-LOC, O, O, B-LOC, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[我, 们, 藏, 有, 一, 册, 1, 9, 4, 5, 年, 6, 月, 油, 印, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[以, 家, 乡, 的, 历, 史, 文, 献, 、, 特, 定, 历, 史, 时, 期, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [当, 希, 望, 工, 程, 救, 助, 的, 百, 万, 儿, 童, 成, 长, 起, ...   \n",
       "1  [藏, 书, 本, 来, 就, 是, 所, 有, 传, 统, 收, 藏, 门, 类, 中, ...   \n",
       "2  [因, 有, 关, 日, 寇, 在, 京, 掠, 夺, 文, 物, 详, 情, ，, 藏, ...   \n",
       "3  [我, 们, 藏, 有, 一, 册, 1, 9, 4, 5, 年, 6, 月, 油, 印, ...   \n",
       "4  [以, 家, 乡, 的, 历, 史, 文, 献, 、, 特, 定, 历, 史, 时, 期, ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, B-LOC, O, O, B-LOC, O, O, O, O, O, O...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look at an observation on the tokens,labels pair and make sure it makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 一 个 统 一 的 中 华 人 民 共 和 国 ， 可 以 实 行 社 会 主 义 和 资 本 主 义 两 种 制 度 ， 这 是 为 了 民 族 、 国 家 的 根 本 利 益 。\n",
      "O O O O O O B-LOC I-LOC I-LOC I-LOC I-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "i = 22\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "\n",
    "print(\" \".join(tokens))\n",
    "print(\" \".join(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model using the **`BertTokenClassifier`** class\n",
    "\n",
    "* We will include an **`ignore_label`** option to exclude the `'O'`,non named entities label, to calculate  `f1`. The non named entities are a huge majority of the labels, and typically `f1` is reported with this class excluded.\n",
    "\n",
    "\n",
    "\n",
    "* We will also use the `'bert-base-chinese'` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = BertTokenClassifier(bert_model='bert-base-chinese',\n",
    "                            epochs=3,\n",
    "                            learning_rate=2e-5,\n",
    "                            train_batch_size=16,\n",
    "                            eval_batch_size=16,\n",
    "                            ignore_label=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue that we need to be mindful of is the max token length in the token lists. \n",
    "There are 2 complications:\n",
    "    \n",
    "* We have a **`max_seq_length`** parameter  with BERT that will dictate how long a token sequence we can handle. All input tokens will be truncaed based on this. The limit on this is 512, but we would like smaller sequences since they are much faster and consume less memory on the GPU. \n",
    "    \n",
    "    \n",
    "* Each token will be tokenized again by the BERT wordpiece tokenizer. This will result in longer token sequences than the input token lists.\n",
    "    \n",
    "Let's check our bert token lengths by running the data through the BERT wordpiece tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert wordpiece tokenizer max token length in train: 100 tokens\n",
      "Bert wordpiece tokenizer max token length in dev: 100 tokens\n",
      "Bert wordpiece tokenizer max token length in test: 83 tokens\n",
      "CPU times: user 17 s, sys: 12 ms, total: 17 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Bert wordpiece tokenizer max token length in train: %d tokens\"% model.get_max_token_len(X_train))\n",
    "print(\"Bert wordpiece tokenizer max token length in dev: %d tokens\"% model.get_max_token_len(X_dev))\n",
    "print(\"Bert wordpiece tokenizer max token length in test: %d tokens\"% model.get_max_token_len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So based on this we will set the max_seq_length to 102 = 100 + 2( for the `'[CLS]'` and `'[SEP]'` tokens that Bert uses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finetune model on train and predict on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenClassifier(bert_model='bert-base-chinese', epochs=3,\n",
      "          eval_batch_size=16, fp16=False, gradient_accumulation_steps=1,\n",
      "          ignore_label=['O'], label_list=None, learning_rate=2e-05,\n",
      "          local_rank=-1, logfile='bert_sklearn.log', loss_scale=0,\n",
      "          max_seq_length=102, num_mlp_hiddens=500, num_mlp_layers=0,\n",
      "          random_state=42, restore_file=None, train_batch_size=16,\n",
      "          use_cuda=True, validation_fraction=0.1, warmup_proportion=0.1)\n",
      "Loading bert-base-chinese model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382072689/382072689 [00:28<00:00, 13584391.81B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to linear classifier/regressor\n",
      "train data size: 45593, validation data size: 5065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2850/2850 [30:49<00:00,  1.64it/s, loss=0.0297]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 0.0297, Val loss: 0.0089, Val accy: 99.28%, f1: 96.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2850/2850 [33:36<00:00,  1.54it/s, loss=0.00517]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss: 0.0052, Val loss: 0.0071, Val accy: 99.49%, f1: 97.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2850/2850 [32:43<00:00,  1.27it/s, loss=0.00224]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss: 0.0022, Val loss: 0.0074, Val accy: 99.53%, f1: 97.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/5 [00:00<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev f1: 96.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/5 [00:00<?, ?it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1: 96.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       1.00      0.98      0.99        45\n",
      "       B-ORG       0.80      1.00      0.89         8\n",
      "       B-PER       0.92      0.92      0.92        25\n",
      "       I-LOC       0.99      1.00      0.99        72\n",
      "       I-ORG       0.88      1.00      0.94        29\n",
      "       I-PER       0.94      0.94      0.94        32\n",
      "           O       1.00      1.00      1.00      2083\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2294\n",
      "   macro avg       0.93      0.98      0.95      2294\n",
      "weighted avg       0.99      0.99      0.99      2294\n",
      "\n",
      "CPU times: user 1h 1min 13s, sys: 41min 12s, total: 1h 42min 25s\n",
      "Wall time: 1h 42min 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set max_seq_length\n",
    "model.max_seq_length = 102\n",
    "print(model)\n",
    "\n",
    "# finetune model on train data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# score model on dev data\n",
    "f1_dev = model.score(X_dev, y_dev)\n",
    "print(\"Dev f1: %0.02f\"%(f1_dev))\n",
    "\n",
    "# score model on test data\n",
    "f1_test = model.score(X_test, y_test)\n",
    "print(\"Test f1: %0.02f\"%(f1_test))\n",
    "\n",
    "# get predictions on test data\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# calculate the probability of each class\n",
    "y_probs = model.predict_proba(X_test)\n",
    "\n",
    "# print report on classifier stats\n",
    "print(classification_report(flatten(y_test), flatten(y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want span level stats we can run the original [perl script](https://www.clips.uantwerpen.be/conll2003/ner/bin/conlleval) to evaluate the results of processing the `CoNLL-2000/2003 shared task`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 2294 tokens with 78 phrases; found: 79 phrases; correct: 71.\r\n",
      "accuracy:  99.43%; precision:  89.87%; recall:  91.03%; FB1:  90.45\r\n",
      "              LOC: precision:  97.73%; recall:  95.56%; FB1:  96.63  44\r\n",
      "              ORG: precision:  80.00%; recall: 100.00%; FB1:  88.89  10\r\n",
      "              PER: precision:  80.00%; recall:  80.00%; FB1:  80.00  25\r\n"
     ]
    }
   ],
   "source": [
    "# write out predictions to file for conlleval.pl\n",
    "iter_zip = zip(flatten(X_test),flatten(y_test),flatten(y_preds))\n",
    "preds = [\" \".join([token, y, y_pred]) for token, y, y_pred in iter_zip]\n",
    "with open(\"preds.txt\",'w') as f:\n",
    "    for x in preds:\n",
    "        f.write(str(x)+'\\n') \n",
    "\n",
    "# run conlleval perl script \n",
    "!perl ./conlleval.pl < preds.txt\n",
    "!rm preds.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at the example from the test set we looked at before and compare the predicted tags with the actuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   token  label predict\n",
      "0      在      O       O\n",
      "1      一      O       O\n",
      "2      个      O       O\n",
      "3      统      O       O\n",
      "4      一      O       O\n",
      "5      的      O       O\n",
      "6      中  B-LOC   B-LOC\n",
      "7      华  I-LOC   I-LOC\n",
      "8      人  I-LOC   I-LOC\n",
      "9      民  I-LOC   I-LOC\n",
      "10     共  I-LOC   I-LOC\n",
      "11     和  I-LOC   I-LOC\n",
      "12     国  I-LOC   I-LOC\n",
      "13     ，      O       O\n",
      "14     可      O       O\n",
      "15     以      O       O\n",
      "16     实      O       O\n",
      "17     行      O       O\n",
      "18     社      O       O\n",
      "19     会      O       O\n",
      "20     主      O       O\n",
      "21     义      O       O\n",
      "22     和      O       O\n",
      "23     资      O       O\n",
      "24     本      O       O\n",
      "25     主      O       O\n",
      "26     义      O       O\n",
      "27     两      O       O\n",
      "28     种      O       O\n",
      "29     制      O       O\n",
      "30     度      O       O\n",
      "31     ，      O       O\n",
      "32     这      O       O\n",
      "33     是      O       O\n",
      "34     为      O       O\n",
      "35     了      O       O\n",
      "36     民      O       O\n",
      "37     族      O       O\n",
      "38     、      O       O\n",
      "39     国      O       O\n",
      "40     家      O       O\n",
      "41     的      O       O\n",
      "42     根      O       O\n",
      "43     本      O       O\n",
      "44     利      O       O\n",
      "45     益      O       O\n",
      "46     。      O       O\n"
     ]
    }
   ],
   "source": [
    "i = 22\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "preds = y_preds[i]\n",
    "probs   = y_probs[i]\n",
    "\n",
    "data = {\"token\": tokens,\"label\": labels,\"predict\": preds}\n",
    "df=pd.DataFrame(data=data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   token  B-LOC  B-ORG  B-PER  I-LOC  I-ORG  I-PER    O\n",
      "0      在   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "1      一   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "2      个   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "3      统   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "4      一   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "5      的   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "6      中   1.00   0.00   0.00   0.00   0.00   0.00 0.00\n",
      "7      华   0.00   0.00   0.00   1.00   0.00   0.00 0.00\n",
      "8      人   0.00   0.00   0.00   1.00   0.00   0.00 0.00\n",
      "9      民   0.00   0.00   0.00   1.00   0.00   0.00 0.00\n",
      "10     共   0.00   0.00   0.00   1.00   0.00   0.00 0.00\n",
      "11     和   0.00   0.00   0.00   1.00   0.00   0.00 0.00\n",
      "12     国   0.00   0.00   0.00   1.00   0.00   0.00 0.00\n",
      "13     ，   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "14     可   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "15     以   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "16     实   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "17     行   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "18     社   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "19     会   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "20     主   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "21     义   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "22     和   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "23     资   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "24     本   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "25     主   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "26     义   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "27     两   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "28     种   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "29     制   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "30     度   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "31     ，   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "32     这   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "33     是   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "34     为   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "35     了   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "36     民   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "37     族   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "38     、   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "39     国   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "40     家   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "41     的   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "42     根   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "43     本   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "44     利   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "45     益   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "46     。   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n"
     ]
    }
   ],
   "source": [
    "# pprint out probs for this obdervation\n",
    "tokens_prob = model.tokens_proba(tokens, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's predict the tags and tag probabilities on some new text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/1 [00:00<?, ?it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   token predicted tags\n",
      "0      乔          B-PER\n",
      "1      治          I-PER\n",
      "2      华          B-PER\n",
      "3      盛          I-PER\n",
      "4      顿          I-PER\n",
      "5      想              O\n",
      "6      访              O\n",
      "7      问              O\n",
      "8      法          B-LOC\n",
      "9      国          I-LOC\n",
      "10     。              O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   token  B-LOC  B-ORG  B-PER  I-LOC  I-ORG  I-PER    O\n",
      "0      乔   0.00   0.00   1.00   0.00   0.00   0.00 0.00\n",
      "1      治   0.00   0.00   0.00   0.00   0.00   1.00 0.00\n",
      "2      华   0.18   0.01   0.61   0.00   0.01   0.19 0.00\n",
      "3      盛   0.00   0.00   0.00   0.01   0.00   0.99 0.00\n",
      "4      顿   0.00   0.00   0.00   0.02   0.00   0.97 0.00\n",
      "5      想   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "6      访   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "7      问   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n",
      "8      法   1.00   0.00   0.00   0.00   0.00   0.00 0.00\n",
      "9      国   0.00   0.00   0.00   1.00   0.00   0.00 0.00\n",
      "10     。   0.00   0.00   0.00   0.00   0.00   0.00 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "text = \"乔治华盛顿想访问法国。\"     \n",
    "\n",
    "tag_predicts  = model.tag_text(text)       \n",
    "prob_predicts = model.tag_text_proba(text)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
