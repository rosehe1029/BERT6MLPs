{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb : Internet Movie Database reviews\n",
    "\n",
    "The IMDb task is a sentiment classification task. It consists of movie reviews collected from IMDB. The training and test set sizes are both 25,000. In addition there is a set of 50,000 unlabeled reviews.\n",
    "\n",
    "See [website](http://ai.stanford.edu/~amaas/data/sentiment/) and [paper](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "sys.path.append(\"../\") \n",
    "from bert_sklearn import BertClassifier\n",
    "from bert_sklearn import load_model\n",
    "\n",
    "DATADIR = \"./aclImdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "tar -xf aclImdb_v1.tar.gz\n",
    "rm aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB train data size: 25000 \n",
      "IMDB unsup data size: 50000 \n",
      "IMDB test data size: 25000 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "IMDB train data size: 25000 \n",
    "IMDB unsup data size: 50000 \n",
    "IMDB test data size: 25000 \n",
    "\"\"\"\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r\"\\\"\", \"\", text)       \n",
    "    return text\n",
    "\n",
    "def slurp(filename):\n",
    "    with open(filename) as f: \n",
    "        data = clean(f.read())\n",
    "    return data\n",
    "\n",
    "def get_imdb_df(datadir,val=None):\n",
    "    data = [(slurp(datadir + filename),val) for filename in os.listdir(datadir)]\n",
    "    return pd.DataFrame(data,columns=['text','label'])\n",
    "\n",
    "def get_imdb_data(train_dir = DATADIR + \"/train\",test_dir = DATADIR + \"/test\",random_state=42 ):\n",
    "\n",
    "    label_list = [0,1]\n",
    "    pos = get_imdb_df(train_dir + \"/pos/\",1)\n",
    "    neg = get_imdb_df(train_dir + \"/neg/\",0)\n",
    "    train = shuffle(pd.concat([pos, neg]),random_state=random_state)\n",
    "    print(\"IMDB train data size: %d \"%(len(train)))\n",
    "    \n",
    "    unsup = get_imdb_df(train_dir + \"/unsup/\")\n",
    "    print(\"IMDB unsup data size: %d \"%(len(unsup)))\n",
    "\n",
    "    pos = get_imdb_df(test_dir + \"/pos/\",1)\n",
    "    neg = get_imdb_df(test_dir + \"/neg/\",0)\n",
    "    test = shuffle(pd.concat([pos, neg]),random_state=random_state)\n",
    "    print(\"IMDB test data size: %d \"%(len(test)))\n",
    "    \n",
    "    return train, test, label_list, unsup\n",
    "\n",
    "train, test, label_list, unsup = get_imdb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>when you add up all the aspects from the movie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11516</th>\n",
       "      <td>Lord Alan Cunningham(Antonio De Teffè)is a nut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9668</th>\n",
       "      <td>I thought it was an extremely clever film. I w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>Granted, I'm not the connoisseur d'horror my p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>I thought it would at least be aesthetically b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "6868   when you add up all the aspects from the movie...      1\n",
       "11516  Lord Alan Cunningham(Antonio De Teffè)is a nut...      0\n",
       "9668   I thought it was an extremely clever film. I w...      1\n",
       "1140   Granted, I'm not the connoisseur d'horror my p...      0\n",
       "1518   I thought it would at least be aesthetically b...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"when you add up all the aspects from the movie---the dancing, singing, acting---the only one who stands out as the best in the cast is Vanessa Williams...her dedication, energy and timeless beauty make Rosie the perfect role for her. Never have i ever seen someone portray Rose with such vibrancy! Vanessa's singing talent shows beautifully with all the songs she performs as Rose and her acting skills never cease to amaze me! Her dancing is so incredible, even if as some people say the choreography was bad---her dancing skills were displayed better than ever before! I'd recommend this version over the '63 just because i find that although lengthy the acting by Vanessa is superb-----not to mention the fact that Jason Alexander and the rest of the cast are very impressive as well (with the exception of Chynna Philips...what in hell were they thinking when they cast her?)All in all I'd say this version is wonderful and I recommend that everyone see this version!\",\n",
       "        1]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each review is much longer than a sentence or two. The Google AI BERT models were trained on sequences of max length 512. Lets look at the performance for max_seq_length equal to  128, 256, and 512.\n",
    "\n",
    "### max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB train data size: 25000 \n",
      "IMDB unsup data size: 50000 \n",
      "IMDB test data size: 25000 \n",
      "Building sklearn classifier...\n",
      "BertClassifier(bert_model='bert-base-uncased', epochs=4, eval_batch_size=8,\n",
      "        fp16=False, gradient_accumulation_steps=1, label_list=None,\n",
      "        learning_rate=2e-05, local_rank=-1, logfile='bert_sklearn.log',\n",
      "        loss_scale=0, max_seq_length=128, num_mlp_hiddens=500,\n",
      "        num_mlp_layers=0, random_state=42, restore_file=None,\n",
      "        train_batch_size=32, use_cuda=True, validation_fraction=0.1,\n",
      "        warmup_proportion=0.1)\n",
      "Loading bert-base-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "train data size: 22500, validation data size: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [09:10<00:00,  1.28it/s, loss=0.396]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss : 0.3956, Val loss: 0.2588, Val accy = 89.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [09:12<00:00,  1.46it/s, loss=0.193]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss : 0.1930, Val loss: 0.2531, Val accy = 90.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [09:11<00:00,  1.45it/s, loss=0.102] \n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss : 0.1019, Val loss: 0.2917, Val accy = 89.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [09:11<00:00,  1.46it/s, loss=0.0698]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train loss : 0.0698, Val loss: 0.3091, Val accy = 89.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.3400, Test accuracy = 89.16%\n",
      "CPU times: user 41min 13s, sys: 17min 47s, total: 59min 1s\n",
      "Wall time: 46min 44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train, test, label_list, unsup = get_imdb_data()\n",
    "\n",
    "X_train = train['text']\n",
    "y_train = train['label']\n",
    "\n",
    "X_test = test['text']\n",
    "y_test = test['label']\n",
    "\n",
    "model = BertClassifier()\n",
    "model.max_seq_length = 128\n",
    "model.learning_rate = 2e-05\n",
    "model.epochs = 4\n",
    "\n",
    "print(model)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "accy = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_seq_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB train data size: 25000 \n",
      "IMDB unsup data size: 50000 \n",
      "IMDB test data size: 25000 \n",
      "Building sklearn classifier...\n",
      "BertClassifier(bert_model='bert-base-uncased', epochs=4, eval_batch_size=8,\n",
      "        fp16=False, gradient_accumulation_steps=1, label_list=None,\n",
      "        learning_rate=2e-05, local_rank=-1, logfile='bert_sklearn.log',\n",
      "        loss_scale=0, max_seq_length=256, num_mlp_hiddens=500,\n",
      "        num_mlp_layers=0, random_state=42, restore_file=None,\n",
      "        train_batch_size=32, use_cuda=True, validation_fraction=0.1,\n",
      "        warmup_proportion=0.1)\n",
      "Loading bert-base-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "train data size: 22500, validation data size: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [14:13<00:00,  1.01s/it, loss=0.336]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss : 0.3360, Val loss: 0.2038, Val accy = 92.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [14:14<00:00,  1.00s/it, loss=0.14] \n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss : 0.1403, Val loss: 0.1911, Val accy = 93.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [14:14<00:00,  1.00s/it, loss=0.0704]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss : 0.0704, Val loss: 0.2216, Val accy = 92.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 704/704 [14:14<00:00,  1.00s/it, loss=0.0474]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train loss : 0.0474, Val loss: 0.2335, Val accy = 93.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.2541, Test accuracy = 92.36%\n",
      "CPU times: user 1h 1min 7s, sys: 31min 55s, total: 1h 33min 2s\n",
      "Wall time: 1h 9min 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train, test, label_list, unsup = get_imdb_data()\n",
    "\n",
    "X_train = train['text']\n",
    "y_train = train['label']\n",
    "\n",
    "X_test = test['text']\n",
    "y_test = test['label']\n",
    "\n",
    "model = BertClassifier()\n",
    "model.max_seq_length = 256\n",
    "model.train_batch_size = 32\n",
    "model.learning_rate = 2e-05\n",
    "model.epochs = 4\n",
    "\n",
    "print(model)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "accy = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB train data size: 25000 \n",
      "IMDB unsup data size: 50000 \n",
      "IMDB test data size: 25000 \n",
      "Building sklearn classifier...\n",
      "BertClassifier(bert_model='bert-base-uncased', epochs=4, eval_batch_size=8,\n",
      "        fp16=False, gradient_accumulation_steps=1, label_list=None,\n",
      "        learning_rate=2e-05, local_rank=-1, logfile='bert_sklearn.log',\n",
      "        loss_scale=0, max_seq_length=512, num_mlp_hiddens=500,\n",
      "        num_mlp_layers=0, random_state=42, restore_file=None,\n",
      "        train_batch_size=16, use_cuda=True, validation_fraction=0.1,\n",
      "        warmup_proportion=0.1)\n",
      "Loading bert-base-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "train data size: 22500, validation data size: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1407/1407 [30:39<00:00,  1.10s/it, loss=0.309]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss : 0.3088, Val loss: 0.1738, Val accy = 93.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1407/1407 [30:39<00:00,  1.10s/it, loss=0.115]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss : 0.1145, Val loss: 0.1770, Val accy = 94.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1407/1407 [30:38<00:00,  1.12s/it, loss=0.0501]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss : 0.0501, Val loss: 0.2188, Val accy = 93.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1407/1407 [31:02<00:00,  1.11s/it, loss=0.0304]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train loss : 0.0304, Val loss: 0.2305, Val accy = 94.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.2223, Test accuracy = 94.04%\n",
      "CPU times: user 2h 6min 43s, sys: 1h 6min 6s, total: 3h 12min 50s\n",
      "Wall time: 2h 22min 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train, test, label_list, unsup = get_imdb_data()\n",
    "\n",
    "X_train = train['text']\n",
    "y_train = train['label']\n",
    "\n",
    "X_test = test['text']\n",
    "y_test = test['label']\n",
    "\n",
    "model = BertClassifier()\n",
    "model.max_seq_length = 512\n",
    "\n",
    "# max_seq_length=512 will use a lot more GPU mem, so I am turning down batch size \n",
    "# and adding gradient accumulation steps\n",
    "model.train_batch_size = 16\n",
    "model_gradient_accumulation_steps = 4\n",
    "\n",
    "model.learning_rate = 2e-05\n",
    "model.epochs = 4\n",
    "\n",
    "print(model)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "accy = model.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
