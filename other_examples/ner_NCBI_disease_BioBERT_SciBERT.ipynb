{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI Disease Corpus\n",
    "\n",
    "The NCBI disease corpus  task is a Named Entity Recognition(NER) task in the biomedical domain. The data is from  a collection of 793 PubMed abstracts with annotations for disease entities. Each token enitity has a `'B-'` or `'I-'` tag indicating if it is the start of the entity or if the token is inside the annotation. The `'O'` tag means the token is not a named entity. See this [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3951655/) for more information\n",
    "\n",
    "\n",
    "NER tasks are token classification tasks where the data consists of features,`X`, and labels,`y`, where:\n",
    "\n",
    "* **`X`** :  is a list of list of tokens \n",
    "\n",
    "\n",
    "* **`y`** :  is a list of list of NER tags\n",
    "\n",
    "\n",
    "We will finetune models from:\n",
    "\n",
    "\n",
    "* [**BERT**](#NCBI_BERT) - this is the standard `BERT` base case model from Google  pretrained on the Books Corpus and English Wikipedia data.\n",
    "\n",
    "\n",
    "\n",
    "* [**SciBERT**](#NCBI_SciBERT) - `SciBERT` is a model from [AllenAI](https://allenai.org/) based on `BERT` but pretrained on scientific text.  For more information on `SciBERT`, see the [github repo](https://github.com/allenai/scibert) and [paper](https://arxiv.org/pdf/1903.10676.pdf).\n",
    "\n",
    "\n",
    "\n",
    "* [**BioBERT**](#NCBI_BioBERT) -  `BioBERT` is a model also based on `BERT` but pretrained on biomedical text.  For more information on `BioBERT`, see the [ github repo](https://github.com/dmis-lab/biobert) and [paper](https://arxiv.org/pdf/1901.08746.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get ncbi data\n",
    "We can get the ncbi data from the allenai github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATADIR=\"NCBI_disease\"\n",
    "if test ! -d \"$DATADIR\";then\n",
    "    echo \"Creating $DATADIR dir\"\n",
    "    mkdir \"$DATADIR\"\n",
    "    cd \"$DATADIR\"\n",
    "    wget https://raw.githubusercontent.com/allenai/scibert/master/data/ner/NCBI-disease/dev.txt\n",
    "    wget https://raw.githubusercontent.com/allenai/scibert/master/data/ner/NCBI-disease/test.txt\n",
    "    wget https://raw.githubusercontent.com/allenai/scibert/master/data/ner/NCBI-disease/train.txt\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and dev data: 6347 sentences, 159670 tokens\n",
      "Test data: 940 sentences, 24497 tokens\n",
      "6347\n",
      "\n",
      "NER tags: ['B-Disease', 'I-Disease', 'O']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "import statistics as stats\n",
    "\n",
    "sys.path.append(\"../\") \n",
    "from bert_sklearn import BertTokenClassifier\n",
    "from bert_sklearn import load_model\n",
    "\n",
    "def read_tsv(filename, quotechar=None):\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))   \n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def read_CoNLL2003_format(filename, idx=3):\n",
    "    \"\"\"Read file in CoNLL-2003 shared task format\"\"\"\n",
    "    \n",
    "    # read file\n",
    "    lines =  open(filename).read().strip()   \n",
    "    \n",
    "    # find sentence-like boundaries\n",
    "    lines = lines.split(\"\\n\\n\")  \n",
    "    \n",
    "     # split on newlines\n",
    "    lines = [line.split(\"\\n\") for line in lines]\n",
    "    \n",
    "    # get tokens\n",
    "    tokens = [[l.split()[0] for l in line] for line in lines]\n",
    "    \n",
    "    # get labels/tags\n",
    "    labels = [[l.split()[idx] for l in line] for line in lines]\n",
    "    \n",
    "    #convert to df\n",
    "    data= {'tokens': tokens, 'labels': labels}\n",
    "    df=pd.DataFrame(data=data)\n",
    "    return df\n",
    "\n",
    "DATADIR = \"NCBI_disease/\"\n",
    "\n",
    "def get_data(trainfile=DATADIR + \"train.txt\",\n",
    "             devfile=DATADIR + \"dev.txt\",\n",
    "             testfile=DATADIR + \"test.txt\"):\n",
    "\n",
    "    train = read_CoNLL2003_format(trainfile, idx=3)    \n",
    "    dev = read_CoNLL2003_format(devfile, idx=3)\n",
    "    \n",
    "    # combine train and dev\n",
    "    train = pd.concat([train, dev])\n",
    "    print(\"Train and dev data: %d sentences, %d tokens\"%(len(train),len(flatten(train.tokens))))\n",
    "\n",
    "    test = read_CoNLL2003_format(testfile, idx=3)\n",
    "    print(\"Test data: %d sentences, %d tokens\"%(len(test),len(flatten(test.tokens))))\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = get_data()\n",
    "\n",
    "X_train, y_train = train.tokens, train.labels\n",
    "X_test, y_test = test.tokens, test.labels\n",
    "\n",
    "print(len(train))\n",
    "\n",
    "label_list = np.unique(flatten(y_train))\n",
    "label_list = list(label_list)\n",
    "print(\"\\nNER tags:\",label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Identification, of, APC2, ,, a, homologue, of...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-Disease, I-Disease,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, adenomatous, polyposis, coli, (, APC, ),...</td>\n",
       "      <td>[O, B-Disease, I-Disease, I-Disease, I-Disease...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Complex, formation, induces, the, rapid, degr...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[In, colon, carcinoma, cells, ,, loss, of, APC...</td>\n",
       "      <td>[O, B-Disease, I-Disease, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Here, ,, we, report, the, identification, and...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [Identification, of, APC2, ,, a, homologue, of...   \n",
       "1  [The, adenomatous, polyposis, coli, (, APC, ),...   \n",
       "2  [Complex, formation, induces, the, rapid, degr...   \n",
       "3  [In, colon, carcinoma, cells, ,, loss, of, APC...   \n",
       "4  [Here, ,, we, report, the, identification, and...   \n",
       "\n",
       "                                              labels  \n",
       "0  [O, O, O, O, O, O, O, O, B-Disease, I-Disease,...  \n",
       "1  [O, B-Disease, I-Disease, I-Disease, I-Disease...  \n",
       "2                        [O, O, O, O, O, O, O, O, O]  \n",
       "3  [O, B-Disease, I-Disease, O, O, O, O, O, O, O,...  \n",
       "4            [O, O, O, O, O, O, O, O, O, O, O, O, O]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at a (tokens, labels) pair for an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token      label\n",
      "0   Occasional          O\n",
      "1     missense          O\n",
      "2    mutations          O\n",
      "3           in          O\n",
      "4          ATM          O\n",
      "5         were          O\n",
      "6         also          O\n",
      "7        found          O\n",
      "8           in          O\n",
      "9       tumour  B-Disease\n",
      "10         DNA          O\n",
      "11        from          O\n",
      "12    patients          O\n",
      "13        with          O\n",
      "14           B  B-Disease\n",
      "15           -  I-Disease\n",
      "16        cell  I-Disease\n",
      "17         non  I-Disease\n",
      "18           -  I-Disease\n",
      "19    Hodgkins  I-Disease\n",
      "20   lymphomas  I-Disease\n",
      "21           (          O\n",
      "22           B  B-Disease\n",
      "23           -  I-Disease\n",
      "24         NHL  I-Disease\n",
      "25           )          O\n",
      "26         and          O\n",
      "27           a          O\n",
      "28           B  B-Disease\n",
      "29           -  I-Disease\n",
      "30         NHL  I-Disease\n",
      "31        cell          O\n",
      "32        line          O\n",
      "33           .          O\n"
     ]
    }
   ],
   "source": [
    "i = 9\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "\n",
    "data = {\"token\": tokens,\"label\": labels}\n",
    "df=pd.DataFrame(data=data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NCBI_BERT'></a>\n",
    "## BERT base cased model\n",
    "\n",
    "We will finetune a BERT base cased model. As in the CoNLL 2003 NER task, we will check the token sequence lengths to set the **`max_seq_length`** parameter in the model:\n",
    "    \n",
    "* The **`max_seq_length`** parameter  will dictate how long a token sequence we can handle. All input token sequences longer than this will be truncated. The limit on this is 512, but we would like smaller sequences since they are much faster and consume less memory on the GPU. \n",
    "    \n",
    "    \n",
    "* Each token will be tokenized again by the BERT wordpiece tokenizer. This will result in longer token sequences than the input token lists. \n",
    "    \n",
    "Let's check our bert wordpiece token lengths by running the data through the BERT wordpiece tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213450/213450 [00:01<00:00, 201278.39B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert wordpiece tokenizer max token length in train: 176 tokens\n",
      "Bert wordpiece tokenizer max token length in test: 155 tokens\n"
     ]
    }
   ],
   "source": [
    "model = BertTokenClassifier('bert-base-cased')\n",
    "print(\"Bert wordpiece tokenizer max token length in train: %d tokens\"% model.get_max_token_len(X_train))\n",
    "print(\"Bert wordpiece tokenizer max token length in test: %d tokens\"% model.get_max_token_len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as long as we set the **`max_seq_length`** to greater than 178 = 176 + 2( for the `'[CLS]'` and `'[SEP]'` delimiter tokens that Bert uses), none of the data will be truncated.\n",
    "\n",
    "If we set the  **`max_seq_length`**  to less than 178, we can still fineune the model, but we will lose the training signal from truncated tokens in the training data. Also at prediction time, we will predict the majority label,`'O'` for any tokens that have been truncated.\n",
    "\n",
    "\n",
    "\n",
    "### finetune BERT\n",
    "\n",
    "We will include an **`ignore_label`** option to exclude the `'O'` = non named entities label, to calculate  `f1`. The non named entities are a huge majority of the labels, and typically `f1` is reported with this class excluded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n",
      "BertTokenClassifier(bert_config_json=None, bert_model='bert-base-cased',\n",
      "          bert_vocab=None, do_lower_case=None, epochs=3,\n",
      "          eval_batch_size=16, fp16=False, from_tf=False,\n",
      "          gradient_accumulation_steps=4, ignore_label=['O'],\n",
      "          label_list=['B-Disease', 'I-Disease', 'O'], learning_rate=3e-05,\n",
      "          local_rank=-1, logfile='bert_sklearn.log', loss_scale=0,\n",
      "          max_seq_length=178, num_mlp_hiddens=500, num_mlp_layers=0,\n",
      "          random_state=42, restore_file=None, train_batch_size=16,\n",
      "          use_cuda=True, validation_fraction=0, warmup_proportion=0.1)\n",
      "Loading bert-base-cased model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404400730/404400730 [02:27<00:00, 2746409.68B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint from  pytorch_model.bin\n",
      "train data size: 6347, validation data size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1587/1587 [04:33<00:00,  6.16it/s, loss=0.0161]\n",
      "Training: 100%|██████████| 1587/1587 [05:40<00:00,  4.45it/s, loss=0.00329]\n",
      "Training: 100%|██████████| 1587/1587 [06:25<00:00,  5.10it/s, loss=0.00133]\n",
      "Predicting:   0%|          | 0/59 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1: 88.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-Disease       0.87      0.89      0.88       960\n",
      "   I-Disease       0.87      0.92      0.89      1087\n",
      "           O       0.99      0.99      0.99     22450\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     24497\n",
      "   macro avg       0.91      0.94      0.92     24497\n",
      "weighted avg       0.98      0.98      0.98     24497\n",
      "\n",
      "CPU times: user 11min 43s, sys: 6min 6s, total: 17min 50s\n",
      "Wall time: 19min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = BertTokenClassifier('bert-base-cased',\n",
    "                            max_seq_length=178,\n",
    "                            epochs=3,\n",
    "                            gradient_accumulation_steps=4,\n",
    "                            learning_rate=3e-5,\n",
    "                            train_batch_size=16,\n",
    "                            eval_batch_size=16,\n",
    "                            validation_fraction=0,                            \n",
    "                            label_list=label_list,\n",
    "                            ignore_label=['O'])\n",
    "\n",
    "print(model)\n",
    "\n",
    "# finetune model on train data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# score model on test data\n",
    "f1_test = model.score(X_test, y_test,'macro')\n",
    "print(\"Test f1: %0.02f\"%(f1_test))\n",
    "\n",
    "# get predictions on test data\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# print report on classifier stats\n",
    "print(classification_report(flatten(y_test), flatten(y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For span level stats, run the original [perl script](https://www.clips.uantwerpen.be/conll2003/ner/bin/conlleval) to evaluate the results of processing the `CoNLL-2000/2003 shared task`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 24497 tokens with 960 phrases; found: 1012 phrases; correct: 839.\r\n",
      "accuracy:  98.36%; precision:  82.91%; recall:  87.40%; FB1:  85.09\r\n",
      "          Disease: precision:  82.91%; recall:  87.40%; FB1:  85.09  1012\r\n"
     ]
    }
   ],
   "source": [
    "# write out predictions to file for conlleval.pl\n",
    "iter_zip = zip(flatten(X_test),flatten(y_test),flatten(y_preds))\n",
    "preds = [\" \".join([token, y, y_pred]) for token, y, y_pred in iter_zip]\n",
    "with open(\"preds.txt\",'w') as f:\n",
    "    for x in preds:\n",
    "        f.write(str(x)+'\\n') \n",
    "\n",
    "# run conlleval perl script \n",
    "!perl ./conlleval.pl < preds.txt\n",
    "!rm preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token      label    predict\n",
      "0   Occasional          O          O\n",
      "1     missense          O          O\n",
      "2    mutations          O          O\n",
      "3           in          O          O\n",
      "4          ATM          O          O\n",
      "5         were          O          O\n",
      "6         also          O          O\n",
      "7        found          O          O\n",
      "8           in          O          O\n",
      "9       tumour  B-Disease  B-Disease\n",
      "10         DNA          O          O\n",
      "11        from          O          O\n",
      "12    patients          O          O\n",
      "13        with          O          O\n",
      "14           B  B-Disease  B-Disease\n",
      "15           -  I-Disease  I-Disease\n",
      "16        cell  I-Disease  I-Disease\n",
      "17         non  I-Disease  I-Disease\n",
      "18           -  I-Disease  I-Disease\n",
      "19    Hodgkins  I-Disease  I-Disease\n",
      "20   lymphomas  I-Disease  I-Disease\n",
      "21           (          O          O\n",
      "22           B  B-Disease  B-Disease\n",
      "23           -  I-Disease  I-Disease\n",
      "24         NHL  I-Disease  I-Disease\n",
      "25           )          O          O\n",
      "26         and          O          O\n",
      "27           a          O          O\n",
      "28           B  B-Disease          O\n",
      "29           -  I-Disease          O\n",
      "30         NHL  I-Disease          O\n",
      "31        cell          O          O\n",
      "32        line          O          O\n",
      "33           .          O          O\n"
     ]
    }
   ],
   "source": [
    "i = 9\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "preds  = y_preds[i]\n",
    "\n",
    "data = {\"token\": tokens,\"label\": labels,\"predict\": preds}\n",
    "df=pd.DataFrame(data=data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token  B-Disease  I-Disease    O\n",
      "0   Occasional       0.00       0.00 1.00\n",
      "1     missense       0.00       0.00 1.00\n",
      "2    mutations       0.00       0.00 1.00\n",
      "3           in       0.00       0.00 1.00\n",
      "4          ATM       0.00       0.00 1.00\n",
      "5         were       0.00       0.00 1.00\n",
      "6         also       0.00       0.00 1.00\n",
      "7        found       0.00       0.00 1.00\n",
      "8           in       0.00       0.00 1.00\n",
      "9       tumour       1.00       0.00 0.00\n",
      "10         DNA       0.00       0.00 1.00\n",
      "11        from       0.00       0.00 1.00\n",
      "12    patients       0.00       0.00 1.00\n",
      "13        with       0.00       0.00 1.00\n",
      "14           B       0.95       0.00 0.05\n",
      "15           -       0.00       0.96 0.04\n",
      "16        cell       0.00       0.95 0.05\n",
      "17         non       0.01       0.99 0.00\n",
      "18           -       0.00       1.00 0.00\n",
      "19    Hodgkins       0.00       1.00 0.00\n",
      "20   lymphomas       0.00       1.00 0.00\n",
      "21           (       0.00       0.00 1.00\n",
      "22           B       0.98       0.00 0.02\n",
      "23           -       0.00       0.98 0.02\n",
      "24         NHL       0.02       0.91 0.08\n",
      "25           )       0.00       0.00 1.00\n",
      "26         and       0.00       0.00 1.00\n",
      "27           a       0.00       0.00 1.00\n",
      "28           B       0.34       0.00 0.66\n",
      "29           -       0.00       0.05 0.95\n",
      "30         NHL       0.01       0.37 0.62\n",
      "31        cell       0.00       0.00 1.00\n",
      "32        line       0.00       0.00 1.00\n",
      "33           .       0.00       0.00 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# calculate the probability of each class\n",
    "y_probs = model.predict_proba(X_test)\n",
    "\n",
    "# pprint out probs for this observation\n",
    "prob = y_probs[i]\n",
    "tokens_prob = model.tokens_proba(tokens, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NCBI_SciBERT'></a>\n",
    "## SciBERT\n",
    "\n",
    "There are 4 SciBERT models available: \n",
    "\n",
    "\n",
    "* `scibert-scivocab-cased`\n",
    "\n",
    "\n",
    "* `scibert-scivocab-uncased` \n",
    "\n",
    "\n",
    "* `scibert-basevocab-cased`\n",
    "\n",
    "\n",
    "* `scibert-basevocab-uncased`\n",
    "\n",
    "\n",
    "\n",
    "See the [`SciBERT` github](https://github.com/allenai/scibert) and [paper](https://arxiv.org/pdf/1903.10676.pdf) for more info.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  finetune `'scibert-basevocab-cased'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n",
      "BertTokenClassifier(bert_config_json=None,\n",
      "          bert_model='scibert-basevocab-cased', bert_vocab=None,\n",
      "          do_lower_case=None, epochs=3, eval_batch_size=16, fp16=False,\n",
      "          from_tf=False, gradient_accumulation_steps=4, ignore_label=['O'],\n",
      "          label_list=None, learning_rate=3e-05, local_rank=-1,\n",
      "          logfile='bert_sklearn.log', loss_scale=0, max_seq_length=178,\n",
      "          num_mlp_hiddens=500, num_mlp_layers=0, random_state=42,\n",
      "          restore_file=None, train_batch_size=16, use_cuda=True,\n",
      "          validation_fraction=0.0, warmup_proportion=0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 403916800/403916800 [00:21<00:00, 18565376.32B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scibert-basevocab-cased model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 403916800/403916800 [00:48<00:00, 8373221.90B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint from  pytorch_model.bin\n",
      "train data size: 6347, validation data size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1587/1587 [04:39<00:00,  5.78it/s, loss=0.0148]\n",
      "Training: 100%|██████████| 1587/1587 [05:52<00:00,  5.25it/s, loss=0.00285]\n",
      "Training: 100%|██████████| 1587/1587 [06:32<00:00,  4.23it/s, loss=0.00116]\n",
      "Predicting:   0%|          | 0/59 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1: 90.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-Disease       0.89      0.92      0.90       960\n",
      "   I-Disease       0.89      0.94      0.91      1087\n",
      "           O       1.00      0.99      0.99     22450\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     24497\n",
      "   macro avg       0.92      0.95      0.94     24497\n",
      "weighted avg       0.99      0.99      0.99     24497\n",
      "\n",
      "CPU times: user 11min 32s, sys: 6min 51s, total: 18min 23s\n",
      "Wall time: 19min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = BertTokenClassifier(bert_model='scibert-basevocab-cased',\n",
    "                            max_seq_length=178,\n",
    "                            epochs=3,\n",
    "                            gradient_accumulation_steps=4,\n",
    "                            learning_rate=3e-5,\n",
    "                            train_batch_size=16,\n",
    "                            eval_batch_size=16,\n",
    "                            validation_fraction=0.,                            \n",
    "                            ignore_label=['O'])\n",
    "\n",
    "print(model)\n",
    "\n",
    "# finetune model on train data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# score model on test data\n",
    "f1_test = model.score(X_test, y_test, 'macro')\n",
    "print(\"Test f1: %0.02f\"%(f1_test))\n",
    "\n",
    "# get predictions on test data\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# print report on classifier stats\n",
    "print(classification_report(flatten(y_test), flatten(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 24497 tokens with 960 phrases; found: 1013 phrases; correct: 871.\r\n",
      "accuracy:  98.65%; precision:  85.98%; recall:  90.73%; FB1:  88.29\r\n",
      "          Disease: precision:  85.98%; recall:  90.73%; FB1:  88.29  1013\r\n"
     ]
    }
   ],
   "source": [
    "# For span level stats, write out predictions to file for conlleval.pl\n",
    "iter_zip = zip(flatten(X_test),flatten(y_test),flatten(y_preds))\n",
    "preds = [\" \".join([token, y, y_pred]) for token, y, y_pred in iter_zip]\n",
    "with open(\"preds.txt\",'w') as f:\n",
    "    for x in preds:\n",
    "        f.write(str(x)+'\\n') \n",
    "\n",
    "# run conlleval perl script \n",
    "!perl ./conlleval.pl < preds.txt\n",
    "!rm preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token      label    predict\n",
      "0   Occasional          O          O\n",
      "1     missense          O          O\n",
      "2    mutations          O          O\n",
      "3           in          O          O\n",
      "4          ATM          O          O\n",
      "5         were          O          O\n",
      "6         also          O          O\n",
      "7        found          O          O\n",
      "8           in          O          O\n",
      "9       tumour  B-Disease  B-Disease\n",
      "10         DNA          O          O\n",
      "11        from          O          O\n",
      "12    patients          O          O\n",
      "13        with          O          O\n",
      "14           B  B-Disease  B-Disease\n",
      "15           -  I-Disease  I-Disease\n",
      "16        cell  I-Disease  I-Disease\n",
      "17         non  I-Disease  I-Disease\n",
      "18           -  I-Disease  I-Disease\n",
      "19    Hodgkins  I-Disease  I-Disease\n",
      "20   lymphomas  I-Disease  I-Disease\n",
      "21           (          O          O\n",
      "22           B  B-Disease  B-Disease\n",
      "23           -  I-Disease  I-Disease\n",
      "24         NHL  I-Disease  I-Disease\n",
      "25           )          O          O\n",
      "26         and          O          O\n",
      "27           a          O          O\n",
      "28           B  B-Disease  B-Disease\n",
      "29           -  I-Disease  I-Disease\n",
      "30         NHL  I-Disease  I-Disease\n",
      "31        cell          O          O\n",
      "32        line          O          O\n",
      "33           .          O          O\n"
     ]
    }
   ],
   "source": [
    "# look at predicted token labels for a test example\n",
    "i = 9\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "preds  = y_preds[i]\n",
    "\n",
    "data = {\"token\": tokens,\"label\": labels,\"predict\": preds}\n",
    "df=pd.DataFrame(data=data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetune  `'scibert-basevocab-uncased'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n",
      "BertTokenClassifier(bert_config_json=None,\n",
      "          bert_model='scibert-basevocab-uncased', bert_vocab=None,\n",
      "          do_lower_case=None, epochs=3, eval_batch_size=16, fp16=False,\n",
      "          from_tf=False, gradient_accumulation_steps=4, ignore_label=['O'],\n",
      "          label_list=['B-Disease', 'I-Disease', 'O'], learning_rate=3e-05,\n",
      "          local_rank=-1, logfile='bert_sklearn.log', loss_scale=0,\n",
      "          max_seq_length=178, num_mlp_hiddens=500, num_mlp_layers=0,\n",
      "          random_state=42, restore_file=None, train_batch_size=16,\n",
      "          use_cuda=True, validation_fraction=0.0, warmup_proportion=0.1)\n",
      "Loading scibert-basevocab-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint from  pytorch_model.bin\n",
      "train data size: 6347, validation data size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1587/1587 [04:36<00:00,  5.99it/s, loss=0.0148]\n",
      "Training: 100%|██████████| 1587/1587 [05:56<00:00,  4.80it/s, loss=0.00311]\n",
      "Training: 100%|██████████| 1587/1587 [06:45<00:00,  3.42it/s, loss=0.00128]\n",
      "Predicting:   0%|          | 0/59 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1: 89.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-Disease       0.87      0.91      0.89       960\n",
      "   I-Disease       0.87      0.92      0.90      1087\n",
      "           O       1.00      0.99      0.99     22450\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     24497\n",
      "   macro avg       0.91      0.94      0.93     24497\n",
      "weighted avg       0.98      0.98      0.98     24497\n",
      "\n",
      "CPU times: user 11min 44s, sys: 6min 27s, total: 18min 12s\n",
      "Wall time: 18min 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BertTokenClassifier(bert_model='scibert-basevocab-uncased',\n",
    "                            max_seq_length=178,\n",
    "                            epochs=3,\n",
    "                            gradient_accumulation_steps=4,\n",
    "                            learning_rate=3e-5,\n",
    "                            train_batch_size=16,\n",
    "                            eval_batch_size=16,\n",
    "                            validation_fraction=0.,                            \n",
    "                            label_list=label_list,\n",
    "                            ignore_label=['O'])\n",
    "\n",
    "print(model)\n",
    "\n",
    "# finetune model on train data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# score model on test data\n",
    "f1_test = model.score(X_test, y_test, 'macro')\n",
    "print(\"Test f1: %0.02f\"%(f1_test))\n",
    "\n",
    "# get predictions on test data\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# print report on classifier stats\n",
    "print(classification_report(flatten(y_test), flatten(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 24497 tokens with 960 phrases; found: 1028 phrases; correct: 853.\r\n",
      "accuracy:  98.43%; precision:  82.98%; recall:  88.85%; FB1:  85.81\r\n",
      "          Disease: precision:  82.98%; recall:  88.85%; FB1:  85.81  1028\r\n"
     ]
    }
   ],
   "source": [
    "# For span level stats, write out predictions to file for conlleval.pl\n",
    "iter_zip = zip(flatten(X_test),flatten(y_test),flatten(y_preds))\n",
    "preds = [\" \".join([token, y, y_pred]) for token, y, y_pred in iter_zip]\n",
    "with open(\"preds.txt\",'w') as f:\n",
    "    for x in preds:\n",
    "        f.write(str(x)+'\\n') \n",
    "\n",
    "# run conlleval perl script \n",
    "!perl ./conlleval.pl < preds.txt\n",
    "!rm preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token      label    predict\n",
      "0   Occasional          O          O\n",
      "1     missense          O          O\n",
      "2    mutations          O          O\n",
      "3           in          O          O\n",
      "4          ATM          O          O\n",
      "5         were          O          O\n",
      "6         also          O          O\n",
      "7        found          O          O\n",
      "8           in          O          O\n",
      "9       tumour  B-Disease  B-Disease\n",
      "10         DNA          O          O\n",
      "11        from          O          O\n",
      "12    patients          O          O\n",
      "13        with          O          O\n",
      "14           B  B-Disease  B-Disease\n",
      "15           -  I-Disease  I-Disease\n",
      "16        cell  I-Disease  I-Disease\n",
      "17         non  I-Disease  I-Disease\n",
      "18           -  I-Disease  I-Disease\n",
      "19    Hodgkins  I-Disease  I-Disease\n",
      "20   lymphomas  I-Disease  I-Disease\n",
      "21           (          O          O\n",
      "22           B  B-Disease  B-Disease\n",
      "23           -  I-Disease  I-Disease\n",
      "24         NHL  I-Disease  I-Disease\n",
      "25           )          O          O\n",
      "26         and          O          O\n",
      "27           a          O          O\n",
      "28           B  B-Disease  B-Disease\n",
      "29           -  I-Disease  I-Disease\n",
      "30         NHL  I-Disease  I-Disease\n",
      "31        cell          O          O\n",
      "32        line          O          O\n",
      "33           .          O          O\n"
     ]
    }
   ],
   "source": [
    "# look at predicted token labels for a test example\n",
    "i = 9\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "preds  = y_preds[i]\n",
    "\n",
    "data = {\"token\": tokens,\"label\": labels,\"predict\": preds}\n",
    "df=pd.DataFrame(data=data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetune  `'scibert-scivocab-cased'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n",
      "BertTokenClassifier(bert_config_json=None,\n",
      "          bert_model='scibert-scivocab-cased', bert_vocab=None,\n",
      "          do_lower_case=None, epochs=3, eval_batch_size=16, fp16=False,\n",
      "          from_tf=False, gradient_accumulation_steps=4, ignore_label=['O'],\n",
      "          label_list=['B-Disease', 'I-Disease', 'O'], learning_rate=3e-05,\n",
      "          local_rank=-1, logfile='bert_sklearn.log', loss_scale=0,\n",
      "          max_seq_length=178, num_mlp_hiddens=500, num_mlp_layers=0,\n",
      "          random_state=42, restore_file=None, train_batch_size=16,\n",
      "          use_cuda=True, validation_fraction=0.0, warmup_proportion=0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410521600/410521600 [00:37<00:00, 10961204.04B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scibert-scivocab-cased model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410521600/410521600 [00:20<00:00, 20012169.06B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint from  pytorch_model.bin\n",
      "train data size: 6347, validation data size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1587/1587 [04:41<00:00,  5.53it/s, loss=0.0123]\n",
      "Training: 100%|██████████| 1587/1587 [05:59<00:00,  4.37it/s, loss=0.00281]\n",
      "Training: 100%|██████████| 1587/1587 [06:34<00:00,  4.41it/s, loss=0.00111]\n",
      "Predicting:   0%|          | 0/59 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1: 90.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-Disease       0.89      0.92      0.90       960\n",
      "   I-Disease       0.89      0.94      0.91      1087\n",
      "           O       1.00      0.99      0.99     22450\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     24497\n",
      "   macro avg       0.92      0.95      0.93     24497\n",
      "weighted avg       0.99      0.99      0.99     24497\n",
      "\n",
      "CPU times: user 11min 30s, sys: 6min 57s, total: 18min 28s\n",
      "Wall time: 19min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = BertTokenClassifier(bert_model='scibert-scivocab-cased',\n",
    "                            max_seq_length=178,\n",
    "                            epochs=3,\n",
    "                            gradient_accumulation_steps=4,\n",
    "                            learning_rate=3e-5,\n",
    "                            train_batch_size=16,\n",
    "                            eval_batch_size=16,\n",
    "                            validation_fraction=0.,                            \n",
    "                            label_list=label_list,\n",
    "                            ignore_label=['O'])\n",
    "\n",
    "print(model)\n",
    "\n",
    "# finetune model on train data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# score model on test data\n",
    "f1_test = model.score(X_test, y_test,'macro')\n",
    "print(\"Test f1: %0.02f\"%(f1_test))\n",
    "\n",
    "# get predictions on test data\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# print report on classifier stats\n",
    "print(classification_report(flatten(y_test), flatten(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 24497 tokens with 960 phrases; found: 1012 phrases; correct: 865.\r\n",
      "accuracy:  98.62%; precision:  85.47%; recall:  90.10%; FB1:  87.73\r\n",
      "          Disease: precision:  85.47%; recall:  90.10%; FB1:  87.73  1012\r\n"
     ]
    }
   ],
   "source": [
    "# For span level stats, write out predictions to file for conlleval.pl\n",
    "iter_zip = zip(flatten(X_test),flatten(y_test),flatten(y_preds))\n",
    "preds = [\" \".join([token, y, y_pred]) for token, y, y_pred in iter_zip]\n",
    "with open(\"preds.txt\",'w') as f:\n",
    "    for x in preds:\n",
    "        f.write(str(x)+'\\n') \n",
    "\n",
    "# run conlleval perl script \n",
    "!perl ./conlleval.pl < preds.txt\n",
    "!rm preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token      label    predict\n",
      "0   Occasional          O          O\n",
      "1     missense          O          O\n",
      "2    mutations          O          O\n",
      "3           in          O          O\n",
      "4          ATM          O          O\n",
      "5         were          O          O\n",
      "6         also          O          O\n",
      "7        found          O          O\n",
      "8           in          O          O\n",
      "9       tumour  B-Disease  B-Disease\n",
      "10         DNA          O          O\n",
      "11        from          O          O\n",
      "12    patients          O          O\n",
      "13        with          O          O\n",
      "14           B  B-Disease  B-Disease\n",
      "15           -  I-Disease  I-Disease\n",
      "16        cell  I-Disease  I-Disease\n",
      "17         non  I-Disease  I-Disease\n",
      "18           -  I-Disease  I-Disease\n",
      "19    Hodgkins  I-Disease  I-Disease\n",
      "20   lymphomas  I-Disease  I-Disease\n",
      "21           (          O          O\n",
      "22           B  B-Disease  B-Disease\n",
      "23           -  I-Disease  I-Disease\n",
      "24         NHL  I-Disease  I-Disease\n",
      "25           )          O          O\n",
      "26         and          O          O\n",
      "27           a          O          O\n",
      "28           B  B-Disease  B-Disease\n",
      "29           -  I-Disease  I-Disease\n",
      "30         NHL  I-Disease  I-Disease\n",
      "31        cell          O          O\n",
      "32        line          O          O\n",
      "33           .          O          O\n"
     ]
    }
   ],
   "source": [
    "# look at predicted token labels for a test example\n",
    "i = 9\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "preds  = y_preds[i]\n",
    "\n",
    "data = {\"token\": tokens,\"label\": labels,\"predict\": preds}\n",
    "df=pd.DataFrame(data=data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetune `scibert-scivocab-uncased`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n",
      "BertTokenClassifier(bert_config_json=None,\n",
      "          bert_model='scibert-scivocab-uncased', bert_vocab=None,\n",
      "          do_lower_case=None, epochs=3, eval_batch_size=16, fp16=False,\n",
      "          from_tf=False, gradient_accumulation_steps=4, ignore_label=['O'],\n",
      "          label_list=['B-Disease', 'I-Disease', 'O'], learning_rate=3e-05,\n",
      "          local_rank=-1, logfile='bert_sklearn.log', loss_scale=0,\n",
      "          max_seq_length=178, num_mlp_hiddens=500, num_mlp_layers=0,\n",
      "          random_state=42, restore_file=None, train_batch_size=16,\n",
      "          use_cuda=True, validation_fraction=0.0, warmup_proportion=0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410593280/410593280 [00:25<00:00, 16160852.87B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scibert-scivocab-uncased model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 410593280/410593280 [00:36<00:00, 11296127.79B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint from  pytorch_model.bin\n",
      "train data size: 6347, validation data size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1587/1587 [05:19<00:00,  4.80it/s, loss=0.0149]\n",
      "Training: 100%|██████████| 1587/1587 [06:35<00:00,  5.00it/s, loss=0.00282]\n",
      "Training: 100%|██████████| 1587/1587 [06:56<00:00,  4.73it/s, loss=0.00117]\n",
      "Predicting:   0%|          | 0/59 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1: 90.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-Disease       0.88      0.92      0.90       960\n",
      "   I-Disease       0.88      0.93      0.90      1087\n",
      "           O       1.00      0.99      0.99     22450\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     24497\n",
      "   macro avg       0.92      0.95      0.93     24497\n",
      "weighted avg       0.99      0.99      0.99     24497\n",
      "\n",
      "CPU times: user 12min 37s, sys: 7min 38s, total: 20min 15s\n",
      "Wall time: 20min 45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = BertTokenClassifier(bert_model='scibert-scivocab-uncased',\n",
    "                            max_seq_length=178,\n",
    "                            epochs=3,\n",
    "                            gradient_accumulation_steps=4,\n",
    "                            learning_rate=3e-5,\n",
    "                            train_batch_size=16,\n",
    "                            eval_batch_size=16,\n",
    "                            validation_fraction=0.,                            \n",
    "                            label_list=label_list,\n",
    "                            ignore_label=['O'])\n",
    "\n",
    "print(model)\n",
    "\n",
    "# finetune model on train data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# score model on test data\n",
    "f1_test = model.score(X_test, y_test,'macro')\n",
    "print(\"Test f1: %0.02f\"%(f1_test))\n",
    "\n",
    "# get predictions on test data\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# print report on classifier stats\n",
    "print(classification_report(flatten(y_test), flatten(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 24497 tokens with 960 phrases; found: 1034 phrases; correct: 872.\r\n",
      "accuracy:  98.59%; precision:  84.33%; recall:  90.83%; FB1:  87.46\r\n",
      "          Disease: precision:  84.33%; recall:  90.83%; FB1:  87.46  1034\r\n"
     ]
    }
   ],
   "source": [
    "# For span level stats, write out predictions to file for conlleval.pl\n",
    "iter_zip = zip(flatten(X_test),flatten(y_test),flatten(y_preds))\n",
    "preds = [\" \".join([token, y, y_pred]) for token, y, y_pred in iter_zip]\n",
    "with open(\"preds.txt\",'w') as f:\n",
    "    for x in preds:\n",
    "        f.write(str(x)+'\\n') \n",
    "\n",
    "# run conlleval perl script \n",
    "!perl ./conlleval.pl < preds.txt\n",
    "!rm preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token      label    predict\n",
      "0   Occasional          O          O\n",
      "1     missense          O          O\n",
      "2    mutations          O          O\n",
      "3           in          O          O\n",
      "4          ATM          O          O\n",
      "5         were          O          O\n",
      "6         also          O          O\n",
      "7        found          O          O\n",
      "8           in          O          O\n",
      "9       tumour  B-Disease  B-Disease\n",
      "10         DNA          O          O\n",
      "11        from          O          O\n",
      "12    patients          O          O\n",
      "13        with          O          O\n",
      "14           B  B-Disease  B-Disease\n",
      "15           -  I-Disease  I-Disease\n",
      "16        cell  I-Disease  I-Disease\n",
      "17         non  I-Disease  I-Disease\n",
      "18           -  I-Disease  I-Disease\n",
      "19    Hodgkins  I-Disease  I-Disease\n",
      "20   lymphomas  I-Disease  I-Disease\n",
      "21           (          O          O\n",
      "22           B  B-Disease  B-Disease\n",
      "23           -  I-Disease  I-Disease\n",
      "24         NHL  I-Disease  I-Disease\n",
      "25           )          O          O\n",
      "26         and          O          O\n",
      "27           a          O          O\n",
      "28           B  B-Disease  B-Disease\n",
      "29           -  I-Disease  I-Disease\n",
      "30         NHL  I-Disease  I-Disease\n",
      "31        cell          O          O\n",
      "32        line          O          O\n",
      "33           .          O          O\n"
     ]
    }
   ],
   "source": [
    "# look at predicted token labels for a test example\n",
    "i = 9\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "preds  = y_preds[i]\n",
    "\n",
    "data = {\"token\": tokens,\"label\": labels,\"predict\": preds}\n",
    "df=pd.DataFrame(data=data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NCBI_BioBERT'></a>\n",
    "\n",
    "## BioBERT\n",
    "\n",
    "There are 4 **`BioBERT`** models available:\n",
    "\n",
    "* `'biobert-v1.0-pmc-base-cased'`\n",
    "\n",
    "\n",
    "* `'biobert-v1.0-pubmed-base-cased'`\n",
    "\n",
    "\n",
    "* `'biobert-v1.0-pubmed-pmc-base-cased'` \n",
    "\n",
    "\n",
    "* `'biobert-v1.1-pubmed-base-cased'` \n",
    "\n",
    "See [BioBERT github](https://github.com/dmis-lab/biobert) and [paper](https://arxiv.org/pdf/1901.08746.pdf)  for more info.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  finetune `'biobert-v1.0-pmc-base-cased'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n",
      "BertTokenClassifier(bert_config_json=None,\n",
      "          bert_model='biobert-v1.0-pmc-base-cased', bert_vocab=None,\n",
      "          do_lower_case=None, epochs=3, eval_batch_size=16, fp16=False,\n",
      "          from_tf=False, gradient_accumulation_steps=4, ignore_label=['O'],\n",
      "          label_list=['B-Disease', 'I-Disease', 'O'], learning_rate=3e-05,\n",
      "          local_rank=-1, logfile='bert_sklearn.log', loss_scale=0,\n",
      "          max_seq_length=178, num_mlp_hiddens=500, num_mlp_layers=0,\n",
      "          random_state=42, restore_file=None, train_batch_size=16,\n",
      "          use_cuda=True, validation_fraction=0.0, warmup_proportion=0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 402110819/402110819 [00:24<00:00, 16241433.83B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading biobert-v1.0-pmc-base-cased model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 402110819/402110819 [01:04<00:00, 6204009.53B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to linear classifier/regressor\n",
      "Loading Tensorflow checkpoint from  biobert_model.ckpt\n",
      "train data size: 6347, validation data size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1587/1587 [05:09<00:00,  5.60it/s, loss=0.0153]\n",
      "Training: 100%|██████████| 1587/1587 [06:41<00:00,  4.00it/s, loss=0.00283]\n",
      "Training: 100%|██████████| 1587/1587 [07:02<00:00,  3.82it/s, loss=0.00109]\n",
      "Predicting:   0%|          | 0/59 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1: 90.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-Disease       0.88      0.91      0.90       960\n",
      "   I-Disease       0.89      0.93      0.91      1087\n",
      "           O       1.00      0.99      0.99     22450\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     24497\n",
      "   macro avg       0.92      0.95      0.93     24497\n",
      "weighted avg       0.99      0.99      0.99     24497\n",
      "\n",
      "CPU times: user 12min 31s, sys: 7min 47s, total: 20min 19s\n",
      "Wall time: 21min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = BertTokenClassifier('biobert-v1.0-pmc-base-cased',\n",
    "                            max_seq_length=178,\n",
    "                            epochs=3,\n",
    "                            gradient_accumulation_steps=4,\n",
    "                            learning_rate=3e-5,\n",
    "                            train_batch_size=16,\n",
    "                            eval_batch_size=16,\n",
    "                            validation_fraction=0.0,                            \n",
    "                            label_list=label_list,\n",
    "                            ignore_label=['O'])\n",
    "\n",
    "print(model)\n",
    "\n",
    "# finetune model on train data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# score model on test data\n",
    "f1_test = model.score(X_test, y_test,'macro')\n",
    "print(\"Test f1: %0.02f\"%(f1_test))\n",
    "\n",
    "# get predictions on test data\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# print report on classifier stats\n",
    "print(classification_report(flatten(y_test), flatten(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 24497 tokens with 960 phrases; found: 1023 phrases; correct: 860.\r\n",
      "accuracy:  98.57%; precision:  84.07%; recall:  89.58%; FB1:  86.74\r\n",
      "          Disease: precision:  84.07%; recall:  89.58%; FB1:  86.74  1023\r\n"
     ]
    }
   ],
   "source": [
    "# For span level stats, write out predictions to file for conlleval.pl\n",
    "iter_zip = zip(flatten(X_test),flatten(y_test),flatten(y_preds))\n",
    "preds = [\" \".join([token, y, y_pred]) for token, y, y_pred in iter_zip]\n",
    "with open(\"preds.txt\",'w') as f:\n",
    "    for x in preds:\n",
    "        f.write(str(x)+'\\n') \n",
    "\n",
    "# run conlleval perl script \n",
    "!perl ./conlleval.pl < preds.txt\n",
    "!rm preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token      label    predict\n",
      "0   Occasional          O          O\n",
      "1     missense          O          O\n",
      "2    mutations          O          O\n",
      "3           in          O          O\n",
      "4          ATM          O          O\n",
      "5         were          O          O\n",
      "6         also          O          O\n",
      "7        found          O          O\n",
      "8           in          O          O\n",
      "9       tumour  B-Disease  B-Disease\n",
      "10         DNA          O          O\n",
      "11        from          O          O\n",
      "12    patients          O          O\n",
      "13        with          O          O\n",
      "14           B  B-Disease  B-Disease\n",
      "15           -  I-Disease  I-Disease\n",
      "16        cell  I-Disease  I-Disease\n",
      "17         non  I-Disease  I-Disease\n",
      "18           -  I-Disease  I-Disease\n",
      "19    Hodgkins  I-Disease  I-Disease\n",
      "20   lymphomas  I-Disease  I-Disease\n",
      "21           (          O          O\n",
      "22           B  B-Disease  B-Disease\n",
      "23           -  I-Disease  I-Disease\n",
      "24         NHL  I-Disease  I-Disease\n",
      "25           )          O          O\n",
      "26         and          O          O\n",
      "27           a          O          O\n",
      "28           B  B-Disease  B-Disease\n",
      "29           -  I-Disease  I-Disease\n",
      "30         NHL  I-Disease  I-Disease\n",
      "31        cell          O          O\n",
      "32        line          O          O\n",
      "33           .          O          O\n"
     ]
    }
   ],
   "source": [
    "# look at predicted token labels for a test example\n",
    "i = 9\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "preds  = y_preds[i]\n",
    "\n",
    "data = {\"token\": tokens,\"label\": labels,\"predict\": preds}\n",
    "df=pd.DataFrame(data=data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  finetune `'biobert-v1.0-pubmed-base-cased'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n",
      "BertTokenClassifier(bert_config_json=None,\n",
      "          bert_model='biobert-v1.0-pubmed-base-cased', bert_vocab=None,\n",
      "          do_lower_case=None, epochs=3, eval_batch_size=16, fp16=False,\n",
      "          from_tf=False, gradient_accumulation_steps=4, ignore_label=['O'],\n",
      "          label_list=None, learning_rate=3e-05, local_rank=-1,\n",
      "          logfile='bert_sklearn.log', loss_scale=0, max_seq_length=178,\n",
      "          num_mlp_hiddens=500, num_mlp_layers=0, random_state=42,\n",
      "          restore_file=None, train_batch_size=16, use_cuda=True,\n",
      "          validation_fraction=0.0, warmup_proportion=0.1)\n",
      "Loading biobert-v1.0-pubmed-base-cased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "Loading Tensorflow checkpoint from  biobert_model.ckpt\n",
      "train data size: 6347, validation data size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1587/1587 [05:51<00:00,  4.33it/s, loss=0.0153]\n",
      "Training: 100%|██████████| 1587/1587 [07:03<00:00,  4.24it/s, loss=0.00263]\n",
      "Training: 100%|██████████| 1587/1587 [06:57<00:00,  4.35it/s, loss=0.00116]\n",
      "Predicting:   0%|          | 0/59 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1: 90.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-Disease       0.89      0.92      0.90       960\n",
      "   I-Disease       0.89      0.94      0.91      1087\n",
      "           O       1.00      0.99      0.99     22450\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     24497\n",
      "   macro avg       0.93      0.95      0.94     24497\n",
      "weighted avg       0.99      0.99      0.99     24497\n",
      "\n",
      "CPU times: user 12min 35s, sys: 8min 8s, total: 20min 43s\n",
      "Wall time: 20min 41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = BertTokenClassifier('biobert-v1.0-pubmed-base-cased',\n",
    "                            max_seq_length=178,\n",
    "                            epochs=3,\n",
    "                            gradient_accumulation_steps=4,\n",
    "                            learning_rate=3e-5,\n",
    "                            train_batch_size=16,\n",
    "                            eval_batch_size=16,\n",
    "                            validation_fraction=0.0,                            \n",
    "                            #label_list=label_list,\n",
    "                            ignore_label=['O'])\n",
    "\n",
    "print(model)\n",
    "\n",
    "# finetune model on train data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# score model on test data\n",
    "f1_test = model.score(X_test, y_test, 'macro')\n",
    "print(\"Test f1: %0.02f\"%(f1_test))\n",
    "\n",
    "# get predictions on test data\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# print report on classifier stats\n",
    "print(classification_report(flatten(y_test), flatten(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 24497 tokens with 960 phrases; found: 1009 phrases; correct: 865.\r\n",
      "accuracy:  98.68%; precision:  85.73%; recall:  90.10%; FB1:  87.86\r\n",
      "          Disease: precision:  85.73%; recall:  90.10%; FB1:  87.86  1009\r\n"
     ]
    }
   ],
   "source": [
    "# For span level stats, write out predictions to file for conlleval.pl\n",
    "iter_zip = zip(flatten(X_test),flatten(y_test),flatten(y_preds))\n",
    "preds = [\" \".join([token, y, y_pred]) for token, y, y_pred in iter_zip]\n",
    "with open(\"preds.txt\",'w') as f:\n",
    "    for x in preds:\n",
    "        f.write(str(x)+'\\n') \n",
    "\n",
    "# run conlleval perl script \n",
    "!perl ./conlleval.pl < preds.txt\n",
    "!rm preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token      label    predict\n",
      "0   Occasional          O          O\n",
      "1     missense          O          O\n",
      "2    mutations          O          O\n",
      "3           in          O          O\n",
      "4          ATM          O          O\n",
      "5         were          O          O\n",
      "6         also          O          O\n",
      "7        found          O          O\n",
      "8           in          O          O\n",
      "9       tumour  B-Disease  B-Disease\n",
      "10         DNA          O          O\n",
      "11        from          O          O\n",
      "12    patients          O          O\n",
      "13        with          O          O\n",
      "14           B  B-Disease  B-Disease\n",
      "15           -  I-Disease  I-Disease\n",
      "16        cell  I-Disease  I-Disease\n",
      "17         non  I-Disease  I-Disease\n",
      "18           -  I-Disease  I-Disease\n",
      "19    Hodgkins  I-Disease  I-Disease\n",
      "20   lymphomas  I-Disease  I-Disease\n",
      "21           (          O          O\n",
      "22           B  B-Disease  B-Disease\n",
      "23           -  I-Disease  I-Disease\n",
      "24         NHL  I-Disease  I-Disease\n",
      "25           )          O          O\n",
      "26         and          O          O\n",
      "27           a          O          O\n",
      "28           B  B-Disease  B-Disease\n",
      "29           -  I-Disease  I-Disease\n",
      "30         NHL  I-Disease  I-Disease\n",
      "31        cell          O          O\n",
      "32        line          O          O\n",
      "33           .          O          O\n"
     ]
    }
   ],
   "source": [
    "# look at predicted token labels for a test example\n",
    "i = 9\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "preds  = y_preds[i]\n",
    "\n",
    "data = {\"token\": tokens,\"label\": labels,\"predict\": preds}\n",
    "df=pd.DataFrame(data=data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetune `'biobert-v1.0-pubmed-pmc-base-cased'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n",
      "BertTokenClassifier(bert_config_json=None,\n",
      "          bert_model='biobert-v1.0-pubmed-pmc-base-cased', bert_vocab=None,\n",
      "          do_lower_case=None, epochs=3, eval_batch_size=16, fp16=False,\n",
      "          from_tf=False, gradient_accumulation_steps=4, ignore_label=['O'],\n",
      "          label_list=None, learning_rate=3e-05, local_rank=-1,\n",
      "          logfile='bert_sklearn.log', loss_scale=0, max_seq_length=178,\n",
      "          num_mlp_hiddens=500, num_mlp_layers=0, random_state=42,\n",
      "          restore_file=None, train_batch_size=16, use_cuda=True,\n",
      "          validation_fraction=0.0, warmup_proportion=0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 402016728/402016728 [01:01<00:00, 6488488.15B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading biobert-v1.0-pubmed-pmc-base-cased model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 402016728/402016728 [00:23<00:00, 17169864.90B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to linear classifier/regressor\n",
      "Loading Tensorflow checkpoint from  biobert_model.ckpt\n",
      "train data size: 6347, validation data size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1587/1587 [05:26<00:00,  3.71it/s, loss=0.0156]\n",
      "Training: 100%|██████████| 1587/1587 [06:50<00:00,  4.73it/s, loss=0.00273]\n",
      "Training: 100%|██████████| 1587/1587 [07:06<00:00,  4.12it/s, loss=0.0011] \n",
      "Predicting:   0%|          | 0/59 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1: 90.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-Disease       0.89      0.92      0.90       960\n",
      "   I-Disease       0.89      0.94      0.91      1087\n",
      "           O       1.00      0.99      0.99     22450\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     24497\n",
      "   macro avg       0.93      0.95      0.94     24497\n",
      "weighted avg       0.99      0.99      0.99     24497\n",
      "\n",
      "CPU times: user 12min 54s, sys: 7min 53s, total: 20min 48s\n",
      "Wall time: 21min 41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BertTokenClassifier('biobert-v1.0-pubmed-pmc-base-cased',\n",
    "                            max_seq_length=178,\n",
    "                            epochs=3,\n",
    "                            gradient_accumulation_steps=4,\n",
    "                            learning_rate=3e-5,\n",
    "                            train_batch_size=16,\n",
    "                            eval_batch_size=16,\n",
    "                            validation_fraction=0.0,                            \n",
    "                            ignore_label=['O'])\n",
    "\n",
    "print(model)\n",
    "\n",
    "# finetune model on train data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# score model on test data\n",
    "f1_test = model.score(X_test, y_test, 'macro')\n",
    "print(\"Test f1: %0.02f\"%(f1_test))\n",
    "\n",
    "# get predictions on test data\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# print report on classifier stats\n",
    "print(classification_report(flatten(y_test), flatten(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 24497 tokens with 960 phrases; found: 1016 phrases; correct: 872.\r\n",
      "accuracy:  98.68%; precision:  85.83%; recall:  90.83%; FB1:  88.26\r\n",
      "          Disease: precision:  85.83%; recall:  90.83%; FB1:  88.26  1016\r\n"
     ]
    }
   ],
   "source": [
    "# For span level stats, write out predictions to file for conlleval.pl\n",
    "iter_zip = zip(flatten(X_test),flatten(y_test),flatten(y_preds))\n",
    "preds = [\" \".join([token, y, y_pred]) for token, y, y_pred in iter_zip]\n",
    "with open(\"preds.txt\",'w') as f:\n",
    "    for x in preds:\n",
    "        f.write(str(x)+'\\n') \n",
    "\n",
    "# run conlleval perl script \n",
    "!perl ./conlleval.pl < preds.txt\n",
    "!rm preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token      label    predict\n",
      "0   Occasional          O          O\n",
      "1     missense          O          O\n",
      "2    mutations          O          O\n",
      "3           in          O          O\n",
      "4          ATM          O          O\n",
      "5         were          O          O\n",
      "6         also          O          O\n",
      "7        found          O          O\n",
      "8           in          O          O\n",
      "9       tumour  B-Disease  B-Disease\n",
      "10         DNA          O          O\n",
      "11        from          O          O\n",
      "12    patients          O          O\n",
      "13        with          O          O\n",
      "14           B  B-Disease  B-Disease\n",
      "15           -  I-Disease  I-Disease\n",
      "16        cell  I-Disease  I-Disease\n",
      "17         non  I-Disease  I-Disease\n",
      "18           -  I-Disease  I-Disease\n",
      "19    Hodgkins  I-Disease  I-Disease\n",
      "20   lymphomas  I-Disease  I-Disease\n",
      "21           (          O          O\n",
      "22           B  B-Disease  B-Disease\n",
      "23           -  I-Disease  I-Disease\n",
      "24         NHL  I-Disease  I-Disease\n",
      "25           )          O          O\n",
      "26         and          O          O\n",
      "27           a          O          O\n",
      "28           B  B-Disease  B-Disease\n",
      "29           -  I-Disease  I-Disease\n",
      "30         NHL  I-Disease  I-Disease\n",
      "31        cell          O          O\n",
      "32        line          O          O\n",
      "33           .          O          O\n"
     ]
    }
   ],
   "source": [
    "# look at predicted token labels for a test example\n",
    "i = 9\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "preds  = y_preds[i]\n",
    "\n",
    "data = {\"token\": tokens,\"label\": labels,\"predict\": preds}\n",
    "df=pd.DataFrame(data=data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetune `'biobert-v1.1-pubmed-base-cased'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n",
      "BertTokenClassifier(bert_config_json=None,\n",
      "          bert_model='biobert-v1.1-pubmed-base-cased', bert_vocab=None,\n",
      "          do_lower_case=None, epochs=3, eval_batch_size=16, fp16=False,\n",
      "          from_tf=False, gradient_accumulation_steps=4, ignore_label=['O'],\n",
      "          label_list=None, learning_rate=3e-05, local_rank=-1,\n",
      "          logfile='bert_sklearn.log', loss_scale=0, max_seq_length=178,\n",
      "          num_mlp_hiddens=500, num_mlp_layers=0, random_state=42,\n",
      "          restore_file=None, train_batch_size=16, use_cuda=True,\n",
      "          validation_fraction=0.0, warmup_proportion=0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401403346/401403346 [00:43<00:00, 9231261.98B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading biobert-v1.1-pubmed-base-cased model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401403346/401403346 [01:09<00:00, 5774462.83B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to linear classifier/regressor\n",
      "Loading Tensorflow checkpoint from  model.ckpt-1000000\n",
      "train data size: 6347, validation data size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1587/1587 [05:01<00:00,  4.06it/s, loss=0.0149]\n",
      "Training: 100%|██████████| 1587/1587 [06:39<00:00,  3.81it/s, loss=0.00272]\n",
      "Training: 100%|██████████| 1587/1587 [06:59<00:00,  4.17it/s, loss=0.00107]\n",
      "Predicting:   0%|          | 0/59 [00:00<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1: 90.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-Disease       0.88      0.92      0.90       960\n",
      "   I-Disease       0.88      0.94      0.91      1087\n",
      "           O       1.00      0.99      0.99     22450\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     24497\n",
      "   macro avg       0.92      0.95      0.93     24497\n",
      "weighted avg       0.99      0.99      0.99     24497\n",
      "\n",
      "CPU times: user 12min 38s, sys: 7min 32s, total: 20min 11s\n",
      "Wall time: 21min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = BertTokenClassifier('biobert-v1.1-pubmed-base-cased',\n",
    "                            max_seq_length=178,\n",
    "                            epochs=3,\n",
    "                            gradient_accumulation_steps=4,\n",
    "                            learning_rate=3e-5,\n",
    "                            train_batch_size=16,\n",
    "                            eval_batch_size=16,\n",
    "                            validation_fraction=0.0,                            \n",
    "                            ignore_label=['O'])\n",
    "\n",
    "print(model)\n",
    "\n",
    "# finetune model on train data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# score model on test data\n",
    "f1_test = model.score(X_test, y_test, 'macro')\n",
    "print(\"Test f1: %0.02f\"%(f1_test))\n",
    "\n",
    "# get predictions on test data\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# print report on classifier stats\n",
    "print(classification_report(flatten(y_test), flatten(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 24497 tokens with 960 phrases; found: 1018 phrases; correct: 863.\r\n",
      "accuracy:  98.62%; precision:  84.77%; recall:  89.90%; FB1:  87.26\r\n",
      "          Disease: precision:  84.77%; recall:  89.90%; FB1:  87.26  1018\r\n"
     ]
    }
   ],
   "source": [
    "# For span level stats, write out predictions to file for conlleval.pl\n",
    "iter_zip = zip(flatten(X_test),flatten(y_test),flatten(y_preds))\n",
    "preds = [\" \".join([token, y, y_pred]) for token, y, y_pred in iter_zip]\n",
    "with open(\"preds.txt\",'w') as f:\n",
    "    for x in preds:\n",
    "        f.write(str(x)+'\\n') \n",
    "\n",
    "# run conlleval perl script \n",
    "!perl ./conlleval.pl < preds.txt\n",
    "!rm preds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token      label    predict\n",
      "0   Occasional          O          O\n",
      "1     missense          O          O\n",
      "2    mutations          O          O\n",
      "3           in          O          O\n",
      "4          ATM          O          O\n",
      "5         were          O          O\n",
      "6         also          O          O\n",
      "7        found          O          O\n",
      "8           in          O          O\n",
      "9       tumour  B-Disease  B-Disease\n",
      "10         DNA          O          O\n",
      "11        from          O          O\n",
      "12    patients          O          O\n",
      "13        with          O          O\n",
      "14           B  B-Disease  B-Disease\n",
      "15           -  I-Disease  I-Disease\n",
      "16        cell  I-Disease  I-Disease\n",
      "17         non  I-Disease  I-Disease\n",
      "18           -  I-Disease  I-Disease\n",
      "19    Hodgkins  I-Disease  I-Disease\n",
      "20   lymphomas  I-Disease  I-Disease\n",
      "21           (          O          O\n",
      "22           B  B-Disease  B-Disease\n",
      "23           -  I-Disease  I-Disease\n",
      "24         NHL  I-Disease  I-Disease\n",
      "25           )          O          O\n",
      "26         and          O          O\n",
      "27           a          O          O\n",
      "28           B  B-Disease  B-Disease\n",
      "29           -  I-Disease  I-Disease\n",
      "30         NHL  I-Disease  I-Disease\n",
      "31        cell          O          O\n",
      "32        line          O          O\n",
      "33           .          O          O\n"
     ]
    }
   ],
   "source": [
    "# look at predicted token labels for a test example\n",
    "i = 9\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "preds  = y_preds[i]\n",
    "\n",
    "data = {\"token\": tokens,\"label\": labels,\"predict\": preds}\n",
    "df=pd.DataFrame(data=data)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
