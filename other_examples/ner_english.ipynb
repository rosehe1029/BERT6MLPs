{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoNLL-2003(english) Named Entity Recognition (NER)\n",
    "\n",
    "The  **`CoNLL 2003`** shared task consists of data from the Reuters 1996 news corpus with annotations for 4 types of `Named Entities` (persons, locations, organizations, and miscellaneous entities). The data is in a [IOB2](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) format.  Each token enitity has a `'B-'` or `'I-'` tag indicating if it is the start of the entity or if the token is inside the annotation. \n",
    "\n",
    "* **`Person`**: `'B-PER'` and  `'I-PER'`\n",
    "\n",
    "\n",
    "* **`Organization`**: `'B-ORG'` and `'I-ORG'`\n",
    "\n",
    "\n",
    "* **`Location`**: `'B-LOC'`  and `'I-LOC'`\n",
    "\n",
    "\n",
    "* **`Miscellaneous`**: `'B-MISC'` and `'I-MISC'`\n",
    "\n",
    "\n",
    "* **`Other(non-named entity)`**: `'O'`\n",
    "\n",
    "See [website](https://www.clips.uantwerpen.be/conll2003/ner/) and [paper](https://www.clips.uantwerpen.be/conll2003/pdf/14247tjo.pdf) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already tokenized and tagged with NER labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token         POS   chunk  NER\n",
    "#-------------------------------\n",
    "# Despite       IN    B-PP   O\n",
    "# winning       VBG   B-VP   O\n",
    "# the           DT    B-NP   O\n",
    "# Asian         JJ    I-NP   B-MISC\n",
    "# Games         NNPS  I-NP   I-MISC\n",
    "# title         NN    I-NP   O\n",
    "# two           CD    B-NP   O\n",
    "# years         NNS   I-NP   O\n",
    "# ago           RB    B-ADVP O\n",
    "# ,             ,     O      O\n",
    "# Uzbekistan    NNP   B-NP   B-LOC\n",
    "# are           VBP   B-VP   O\n",
    "# in            IN    B-PP   O\n",
    "# the           DT    B-NP   O\n",
    "# finals        NNS   I-NP   O\n",
    "# as            IN    B-SBAR O\n",
    "# outsiders     NNS   B-NP   O\n",
    "# .             .     O      O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the token, the second column is the Part of Speech(POS) tag, the third is syntactic chunk tag, and the fourth is the NER tag.\n",
    "\n",
    "So for the named entity recognition (NER) task the data consists of features:`X`and labels:`y`\n",
    "\n",
    "\n",
    "* **`X`** :  a list of list of tokens \n",
    "\n",
    "\n",
    "* **`y`** :  a list of list of NER tags\n",
    "\n",
    "\n",
    "## get data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATADIR=\"ner_english\"\n",
    "if test ! -d \"$DATADIR\";then\n",
    "    echo \"Creating $DATADIR dir\"\n",
    "    mkdir \"$DATADIR\"\n",
    "    cd \"$DATADIR\"\n",
    "    wget https://raw.githubusercontent.com/mxhofer/Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL/master/data/train.txt\n",
    "    wget https://raw.githubusercontent.com/mxhofer/Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL/master/data/test.txt\n",
    "    wget https://raw.githubusercontent.com/mxhofer/Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL/master/data/dev.txt\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 14987 sentences, 204567 tokens\n",
      "Dev data: 3466 sentences, 51578 tokens\n",
      "Test data: 3684 sentences, 46666 tokens\n",
      "\n",
      "NER tags: ['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train data: 14987 sentences, 204567 tokens\n",
    "Dev data: 3466 sentences, 51578 tokens\n",
    "Test data: 3684 sentences, 46666 tokens\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sys.path.append(\"../\") \n",
    "from bert_sklearn import BertTokenClassifier, load_model\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def read_CoNLL2003_format(filename, idx=3):\n",
    "    \"\"\"Read file in CoNLL-2003 shared task format\"\"\" \n",
    "    \n",
    "    lines =  open(filename).read().strip()\n",
    "    \n",
    "    # find sentence-like boundaries\n",
    "    lines = lines.split(\"\\n\\n\")  \n",
    "    \n",
    "    # throw out -DOCSTART- lines \n",
    "    #lines = [line for line in lines if not line.startswith(\"-DOCSTART-\")]\n",
    "    \n",
    "     # split on newlines\n",
    "    lines = [line.split(\"\\n\") for line in lines]\n",
    "    \n",
    "    # get tokens\n",
    "    tokens = [[l.split()[0] for l in line] for line in lines]\n",
    "    \n",
    "    # get labels/tags\n",
    "    labels = [[l.split()[idx] for l in line] for line in lines]\n",
    "    \n",
    "    data= {'tokens': tokens, 'labels': labels}\n",
    "    df=pd.DataFrame(data=data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "DATADIR = \"./ner_english/\"\n",
    "\n",
    "def get_conll2003_data(trainfile=DATADIR + \"train.txt\",\n",
    "                  devfile=DATADIR + \"dev.txt\",\n",
    "                  testfile=DATADIR + \"test.txt\"):\n",
    "\n",
    "    train = read_CoNLL2003_format(trainfile)\n",
    "    print(\"Train data: %d sentences, %d tokens\"%(len(train),len(flatten(train.tokens))))\n",
    "\n",
    "    dev = read_CoNLL2003_format(devfile)\n",
    "    print(\"Dev data: %d sentences, %d tokens\"%(len(dev),len(flatten(dev.tokens))))\n",
    "\n",
    "    test = read_CoNLL2003_format(testfile)\n",
    "    print(\"Test data: %d sentences, %d tokens\"%(len(test),len(flatten(test.tokens))))\n",
    "    \n",
    "    return train, dev, test\n",
    "\n",
    "train, dev, test = get_conll2003_data()\n",
    "\n",
    "X_train, y_train = train.tokens, train.labels\n",
    "X_dev, y_dev = dev.tokens, dev.labels\n",
    "X_test, y_test = test.tokens, test.labels\n",
    "\n",
    "\n",
    "label_list = np.unique(flatten(y_train))\n",
    "label_list = list(label_list)\n",
    "print(\"\\nNER tags:\",label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-DOCSTART-]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n",
       "      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, European, Commission, said, on, Thursday...</td>\n",
       "      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0                                       [-DOCSTART-]   \n",
       "1  [EU, rejects, German, call, to, boycott, Briti...   \n",
       "2                                 [Peter, Blackburn]   \n",
       "3                             [BRUSSELS, 1996-08-22]   \n",
       "4  [The, European, Commission, said, on, Thursday...   \n",
       "\n",
       "                                              labels  \n",
       "0                                                [O]  \n",
       "1          [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]  \n",
       "2                                     [B-PER, I-PER]  \n",
       "3                                         [B-LOC, O]  \n",
       "4  [O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an observation on the tokens, labels pair :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token   label\n",
      "0        Dutch  B-MISC\n",
      "1      forward       O\n",
      "2       Reggie   B-PER\n",
      "3      Blinker   I-PER\n",
      "4          had       O\n",
      "5          his       O\n",
      "6   indefinite       O\n",
      "7   suspension       O\n",
      "8       lifted       O\n",
      "9           by       O\n",
      "10        FIFA   B-ORG\n",
      "11          on       O\n",
      "12      Friday       O\n",
      "13         and       O\n",
      "14         was       O\n",
      "15         set       O\n",
      "16          to       O\n",
      "17        make       O\n",
      "18         his       O\n",
      "19   Sheffield   B-ORG\n",
      "20   Wednesday   I-ORG\n",
      "21    comeback       O\n",
      "22     against       O\n",
      "23   Liverpool   B-ORG\n",
      "24          on       O\n",
      "25    Saturday       O\n",
      "26           .       O\n"
     ]
    }
   ],
   "source": [
    "i = 152\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "\n",
    "data = {\"token\": tokens,\"label\": labels}\n",
    "df=pd.DataFrame(data=data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model using the **`BertTokenClassifier`** class\n",
    "\n",
    "* We will include an **`ignore_label`** option to exclude the `'O'`,non named entities label, to calculate  `f1`. The non named entities are a huge majority of the labels, and typically `f1` is reported with this class excluded.\n",
    "\n",
    "\n",
    "* We will also use the `'bert-base-cased'` model as casing provides an important signal for NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = BertTokenClassifier(bert_model='bert-base-cased',\n",
    "                            epochs=3,\n",
    "                            learning_rate=5e-5,\n",
    "                            train_batch_size=16,\n",
    "                            eval_batch_size=16,\n",
    "                            validation_fraction=0.05,                            \n",
    "                            label_list=label_list,\n",
    "                            ignore_label=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue that we need to be mindful of is the max token length in the token lists. \n",
    "There are 2 complications:\n",
    "    \n",
    "    \n",
    "* The **`max_seq_length`** parameter in the model will dictate how long a token sequence we can handle. All input token sequences longer than this will be truncated. The limit on this is 512, but we would like smaller sequences since they are much faster and consume less memory on the GPU. \n",
    "    \n",
    "    \n",
    "* Each token will be tokenized again by the BERT wordpiece tokenizer. This will result in longer token sequences than the input token lists. \n",
    "    \n",
    "    \n",
    "Let's check our bert token lengths by running the data through the BERT wordpiece tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213450/213450 [00:00<00:00, 840482.31B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert wordpiece tokenizer max token length in train: 171 tokens\n",
      "Bert wordpiece tokenizer max token length in dev: 149 tokens\n",
      "Bert wordpiece tokenizer max token length in test: 146 tokens\n",
      "CPU times: user 4.24 s, sys: 12 ms, total: 4.25 s\n",
      "Wall time: 4.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Bert wordpiece tokenizer max token length in train: %d tokens\"% model.get_max_token_len(X_train))\n",
    "print(\"Bert wordpiece tokenizer max token length in dev: %d tokens\"% model.get_max_token_len(X_dev))\n",
    "print(\"Bert wordpiece tokenizer max token length in test: %d tokens\"% model.get_max_token_len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as long as we set the **`max_seq_length`** to greater than 173 = 171 + 2( for the `'[CLS]'` and `'[SEP]'` delimiter tokens that Bert uses), none of the data will be truncated.\n",
    "\n",
    "If we set the  **`max_seq_length`**  to less than that, we can still fineune the model but we will lose the training signal from truncated tokens in the training data. Also at prediction time, we will predict the majority label,`'O'` for any tokens that have been truncated.\n",
    "\n",
    "## finetune model on train and predict on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenClassifier(bert_model='bert-base-cased', epochs=3,\n",
      "          eval_batch_size=16, fp16=False, gradient_accumulation_steps=2,\n",
      "          ignore_label=['O'],\n",
      "          label_list=['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'],\n",
      "          learning_rate=5e-05, local_rank=-1, logfile='bert_sklearn.log',\n",
      "          loss_scale=0, max_seq_length=173, num_mlp_hiddens=500,\n",
      "          num_mlp_layers=0, random_state=42, restore_file=None,\n",
      "          train_batch_size=16, use_cuda=True, validation_fraction=0.05,\n",
      "          warmup_proportion=0.1)\n",
      "Loading bert-base-cased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "train data size: 14238, validation data size: 749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1780/1780 [15:03<00:00,  1.92it/s, loss=0.0113]\n",
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 0.0113, Val loss: 0.0034, Val accy: 98.89%, f1: 94.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1780/1780 [15:51<00:00,  2.04it/s, loss=0.00174]\n",
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss: 0.0017, Val loss: 0.0029, Val accy: 99.04%, f1: 95.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1780/1780 [15:36<00:00,  2.00it/s, loss=0.000668]\n",
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss: 0.0007, Val loss: 0.0028, Val accy: 99.28%, f1: 96.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/231 [00:00<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev f1: 96.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/231 [00:00<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1: 91.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.93      0.93      0.93      1668\n",
      "      B-MISC       0.83      0.84      0.84       702\n",
      "       B-ORG       0.90      0.91      0.91      1661\n",
      "       B-PER       0.96      0.97      0.96      1617\n",
      "       I-LOC       0.82      0.93      0.87       257\n",
      "      I-MISC       0.64      0.78      0.70       216\n",
      "       I-ORG       0.87      0.91      0.89       835\n",
      "       I-PER       0.99      0.99      0.99      1156\n",
      "           O       1.00      0.99      0.99     38554\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     46666\n",
      "   macro avg       0.88      0.92      0.90     46666\n",
      "weighted avg       0.98      0.98      0.98     46666\n",
      "\n",
      "CPU times: user 32min 24s, sys: 21min 2s, total: 53min 27s\n",
      "Wall time: 53min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.max_seq_length = 173\n",
    "model.gradient_accumulation_steps = 2\n",
    "print(model)\n",
    "\n",
    "# finetune model on train data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# score model on dev data\n",
    "f1_dev = model.score(X_dev, y_dev)\n",
    "print(\"Dev f1: %0.02f\"%(f1_dev))\n",
    "\n",
    "# score model on test data\n",
    "f1_test = model.score(X_test, y_test)\n",
    "print(\"Test f1: %0.02f\"%(f1_test))\n",
    "\n",
    "# get predictions on test data\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# calculate the probability of each class\n",
    "y_probs = model.predict_proba(X_test)\n",
    "\n",
    "# print report on classifier stats\n",
    "print(classification_report(flatten(y_test), flatten(y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want span level stats, we can run the original [perl script](https://www.clips.uantwerpen.be/conll2003/ner/bin/conlleval) to evaluate the results of processing the `CoNLL-2000/2003 shared task`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46666 tokens with 5648 phrases; found: 5740 phrases; correct: 5173.\r\n",
      "accuracy:  98.15%; precision:  90.12%; recall:  91.59%; FB1:  90.85\r\n",
      "              LOC: precision:  92.24%; recall:  92.69%; FB1:  92.46  1676\r\n",
      "             MISC: precision:  78.07%; recall:  81.62%; FB1:  79.81  734\r\n",
      "              ORG: precision:  87.64%; recall:  90.07%; FB1:  88.84  1707\r\n",
      "              PER: precision:  96.00%; recall:  96.35%; FB1:  96.17  1623\r\n"
     ]
    }
   ],
   "source": [
    "# write out predictions to file for conlleval.pl\n",
    "iter_zip = zip(flatten(X_test),flatten(y_test),flatten(y_preds))\n",
    "preds = [\" \".join([token, y, y_pred]) for token, y, y_pred in iter_zip]\n",
    "with open(\"preds.txt\",'w') as f:\n",
    "    for x in preds:\n",
    "        f.write(str(x)+'\\n') \n",
    "\n",
    "# run conlleval perl script \n",
    "!perl ./conlleval.pl < preds.txt\n",
    "!rm preds.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at the example from the test set we looked at before and compare the predicted tags with the actuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token   label predict\n",
      "0        Dutch  B-MISC  B-MISC\n",
      "1      forward       O       O\n",
      "2       Reggie   B-PER   B-PER\n",
      "3      Blinker   I-PER   I-PER\n",
      "4          had       O       O\n",
      "5          his       O       O\n",
      "6   indefinite       O       O\n",
      "7   suspension       O       O\n",
      "8       lifted       O       O\n",
      "9           by       O       O\n",
      "10        FIFA   B-ORG   B-ORG\n",
      "11          on       O       O\n",
      "12      Friday       O       O\n",
      "13         and       O       O\n",
      "14         was       O       O\n",
      "15         set       O       O\n",
      "16          to       O       O\n",
      "17        make       O       O\n",
      "18         his       O       O\n",
      "19   Sheffield   B-ORG   B-ORG\n",
      "20   Wednesday   I-ORG   I-ORG\n",
      "21    comeback       O       O\n",
      "22     against       O       O\n",
      "23   Liverpool   B-ORG   B-ORG\n",
      "24          on       O       O\n",
      "25    Saturday       O       O\n",
      "26           .       O       O\n"
     ]
    }
   ],
   "source": [
    "i = 152\n",
    "tokens = X_test[i]\n",
    "labels = y_test[i]\n",
    "preds  = y_preds[i]\n",
    "prob   = y_probs[i]\n",
    "\n",
    "data = {\"token\": tokens,\"label\": labels,\"predict\": preds}\n",
    "df=pd.DataFrame(data=data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate tthe probability of each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         token  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER    O\n",
      "0        Dutch   0.00    1.00   0.00   0.00   0.00    0.00   0.00   0.00 0.00\n",
      "1      forward   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "2       Reggie   0.00    0.00   0.00   1.00   0.00    0.00   0.00   0.00 0.00\n",
      "3      Blinker   0.00    0.00   0.00   0.00   0.00    0.00   0.00   1.00 0.00\n",
      "4          had   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "5          his   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "6   indefinite   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "7   suspension   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "8       lifted   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "9           by   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "10        FIFA   0.00    0.00   1.00   0.00   0.00    0.00   0.00   0.00 0.00\n",
      "11          on   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "12      Friday   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "13         and   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "14         was   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "15         set   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "16          to   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "17        make   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "18         his   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "19   Sheffield   0.00    0.00   1.00   0.00   0.00    0.00   0.00   0.00 0.00\n",
      "20   Wednesday   0.00    0.00   0.00   0.00   0.00    0.00   0.99   0.00 0.00\n",
      "21    comeback   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "22     against   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "23   Liverpool   0.00    0.00   1.00   0.00   0.00    0.00   0.00   0.00 0.00\n",
      "24          on   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "25    Saturday   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "26           .   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n"
     ]
    }
   ],
   "source": [
    "# pprint out probs for this observation\n",
    "tokens_prob = model.tokens_proba(tokens, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, predict the tags and tag probabilities on some new text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/1 [00:00<?, ?it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       token predicted tags\n",
      "0  Jefferson          B-PER\n",
      "1      wants              O\n",
      "2         to              O\n",
      "3         go              O\n",
      "4         to              O\n",
      "5     France          B-LOC\n",
      "6          .              O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       token  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER    O\n",
      "0  Jefferson   0.00    0.00   0.00   1.00   0.00    0.00   0.00   0.00 0.00\n",
      "1      wants   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "2         to   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "3         go   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "4         to   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n",
      "5     France   1.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 0.00\n",
      "6          .   0.00    0.00   0.00   0.00   0.00    0.00   0.00   0.00 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "text = \"Jefferson wants to go to France.\"       \n",
    "\n",
    "tag_predicts  = model.tag_text(text)       \n",
    "prob_predicts = model.tag_text_proba(text)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save/load\n",
    "\n",
    "If we want to save the model to disk:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "savepath = \"/data/ner_english.bin\"\n",
    "model.save(savepath)\n",
    "\n",
    "# restore model\n",
    "model = load_model(savepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
